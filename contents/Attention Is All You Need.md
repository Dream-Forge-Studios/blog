---
date: '2023-12-29'
title: 'Attention Is All You Need 논문 리뷰'
categories: ['Large Language']
summary: 'Attention Is All You Need 완벽 이해하기.'
thumbnail: './test.png'
---

<div id="Introduction"></div>

## Introduction

과거 언어 모델링과 기계 번역에서는 순환 신경망(RNN,LSTM,GRU) 모델을 활용하였습니다.

<br>

하지만 순차적 특성은 이전 요소의 처리 결과에 의존하여 시퀀스의 각 부분을 독립적으로 동시에 처리하기 때문에 병렬처리가 어렵고, 특히 긴 시퀀스 길이에서는 메모리 제약으로 인해 배치 처리가 제한됩니다.

**Attention mechanisms**

<br>

Attention mechanisms는 입력 데이터의 모든 부분이 동등한 중요도를 갖지 않는다는 아이디어에서 출발합니다. 모델이 특정 부분에 더 많은 'attention'를 기울여 그 부분의 중요도를 높입니다.

<br>

**Attention mechanisms 작동 원리**

1. Query, Key, Value:

- Query: 현재 타겟 또는 출력 상태를 나타냅니다.
- Key: 입력 데이터의 각 요소를 나타내며, 쿼리와 비교되어 유사도를 계산하는 데 사용됩니다. (Query 부분에서 해당 토큰이 얼마나 중요한지)
- Value: 입력 데이터의 각 요소에 대응하며, 가중치가 적용된 후 최종 출력에 기여합니다. (토큰의 실제 정보)
- 입력 시퀀스의 각 토큰(예: 단어, 문자)마다 고유한 Key와 Value가 할당
- Query는 입력 시퀀스의 각 토큰마다 별도로 생성되지 않고, 대신 현재의 타겟 상태 또는 출력에 대한 Query가 할당 <br> 예) 시퀀스-투-시퀀스 모델(기계번역)에서는 디코더의 각 층마다 하나씩 생성

2. 유사도 계산 및 가중치 할당:

- Query와 각 Key 사이의 유사도를 계산(내적 dot product)하여 attention weight를 결정합니다.
- 이 weight는 입력 데이터의 어느 부분이 현재 출력에 더 중요한지 나타냅니다.
- attention weight는 보통 Softmax 함수를 통해 정규화되어, 모든 weight의 합이 1이 되도록 합니다.
- 이후 Value와 결합하여 context vector를 생성합니다. <br> (현재 출력에 중요성에 따라 입력 데이터의 정보가 반영되도록 하는 계산의 사용)
- 최종적으로 생성된 context vector와 현재의 타겟 상태를 결합하여 최종 출력(예: 번역된 단어, 다음 단어)을 생성하는 데 사용됩니다.

**Transformer의 제안**

<br>

이전에는 attention mechanisms을 순환 신경망과 결합하여 사용하였습니다.

<br>

그런데 Transformer 모델은 순환을 제외하고 전적으로 어텐션 메커니즘에 의존합니다. 이는 입력과 출력 사이의 전역 의존성을 효과적으로 포착할 수 있게 해주며, 병렬 처리를 크게 향상시킵니다.

<br>

Transformer 더 많은 병렬 처리를 가능하게 함으로써, 트레이닝 시간을 단축시키고 번역 품질에서 새로운 최고 기록을 달성할 수 있습니다. 예를 들어, P100 GPU 8개에서 단 12시간만에 트레이닝을 완료할 수 있습니다.

<div id="Related Work"></div>

## Related Work

AI를 사용하여 법에 대한 접근성을 높이고 분쟁 해결을 지원하는 방법은 다양하게 연구되고 있습니다.

- "협상된 합의안에 대한 최선의 대안(BATNA)"을 협상 중에 표시하는 것
- 게임 이론 방법을 사용하여 수용 가능한 합의를 이끌어내는 접근 방법
- 당사자 간의 대화를 구조화하는 방법 
- 당사자들이 사용하는 염증성 언어를 감지하고, 분쟁을 원만하게 해결할 수 있는 대안적인 메시지 전달 방법을 제안
- ChatGPT를 사용하여 중재자가 역할을 수행하는 데 도움이 될 수 있는 관련 질문과 당사자가 가장 중요하게 여기는 사항을 제안하는 가능성을 탐구
- LLMs이 중재자에게 제안을 생성하거나, 심지어 협상에 자동으로 개입하는 능력을 평가
- ODR 맥락에서 판사 관점을 생성하는 모델 개발 기술을 제안

<div id="Example use cases"></div>

## Example use cases

LLMediator 플랫폼은 LLMs의 능력을 활용하여 중재 플랫폼에서 여러 과제를 지원하는 몇 가지 독특한 기능을 포함합니다.

###  F1 - 감정적인 메세지 재구성

협상 상황에서 사용자가 감정적인 메시지를 보내려고 할 때, LLMediator 플랫폼은 이러한 언어를 감지합니다.

<br>

플랫폼은 GPT-4에 의해 생성된 대체 문장 형식을 사용자에게 제안합니다.

<img style="width: 60%; margin-top: 40px;" id="output" src="LLMediator/message.PNG">
빌린 카메라가 부셔진 상황

###  F2 - 중재자를 위한 메시지 초안 제안

중재자는 당사자들이 친근한 해결책에 도달할 수 있도록 격려하는 역할을 합니다.

<br>

협상이 교착 상태에 있거나 결론에 이르지 못했을 때, 중재자의 개입이 중요할 수 있습니다.

<br>

중재자를 위해 GPT-4를 사용하여 이전에 보낸 메시지를 읽고 당사자들을 친근한 해결책으로 부드럽게 안내하는 제안 메시지를 초안합니다.

<img style="width: 60%; margin-top: 40px;" id="output" src="LLMediator/suggestion.PNG">

###  F3 - 자동적으로 개입

일부 상황에서는 모델이 협상에 자동으로 개입하는 것이 타당할 수 있습니다.

<br>

예를 들어, 분쟁 가치가 인간 중재자를 고용하기에는 너무 낮거나, 특정 지역에서 모든 분쟁을 다룰 중재자가 부족한 경우가 이에 해당될 수 있습니다.

<br>

LLMediator는 자동적으로 메시지를 생성하여 당사자들에게 보냈으며, 합의를 장려하기 위해 몇 가지 가능한 옵션을 제안했습니다.

<img style="width: 50%; margin-top: 40px;" id="output" src="LLMediator/automatic.PNG">

<div id="Technical considerations"></div>

## Technical considerations

### Large language model used

시스템에는 OpenAI가 개발한 GPT-4 모델이 사용되었습니다.

<br>

GPT-4는 다양한 작업에서 인상적인 성능을 보여주었으며, Uniform Bar Examination(변호사 시험) 통과와 같은 뛰어난 성과를 달성했습니다.

### F1 - 감정적인 메세지 재구성

**Detect a message requiring intervention**

<br>

GPT-4에게 모든 메시지를 전송하고 염증성 여부를 문의하는 방법은 메시지의 양에 따라 비용이 많이 들고 플랫폼에 지연을 초래할 수 있으며, 다른 당사자에게 메시지를 보내기 전에 분석해야 하므로 사용자에게 혼란을 줄 수 있습니다.

<br>

더 정교한 감정적 메시지 감지 방법은 향후 연구에서 탐구가 필요할 것으로 보입니다.

<br>

**Reformulating the message**

<br>

사용된 prompt:

<br>

"당신은 ODR(온라인 분쟁 해결) 플랫폼입니다. 당사자의 채팅 메시지가 주어졌습니다. 내용은 유지하되, 메시지를 덜 대립적이고 원만한 합의에 더 도움이 되도록 재구성하세요. 재구성된 메시지로 직접 응답하고, 설명하지 마세요."

<br>

목표는 메시지를 덜 대립적이고, 원만한 합의에 더 유도하는 방향으로 만드는 것입니다. 또한 사용자의 요구에 따라 좀 더 방어적 혹은 공격적으로 재구성하는 방법에 대한 연구도 필요합니다.

### F2 - 중재자를 위한 메시지 초안 제안

**Generating the message suggestion**

<br>

사용된 prompt:

<br>

"당신은 중재자입니다. 당신의 목표는 두 당사자의 토론을 양 당사자 모두에게 수용 가능한 원만한 해결책으로 유도하는 것입니다. 당사자들 사이의 이 커뮤니케이션에 응답하세요. 중재자의 역할에 충실하되, 당사자들의 대화를 완성하지 마세요. 중립을 유지하고, 어느 한쪽 당사자의 편을 들지 마세요."

<br>

모델에는 대화에서 가장 최근의 10개 메시지가 맥락으로 제공되며, 중재자가 추가 지시를 입력할 수 있습니다.

### F3 - 자동적으로 개입

이는 매우 흥미롭고 강력한 사용 사례가 될 수 있지만 여러 가지 상당한 위험도 내포하고 있습니다. 따라서 그러한 시스템을 구축하기 전에 상당한 연구가 수행되어야 합니다.

<br>

**Triggers**

- 활동이 없는 기간이 일정 시간 지속될 때
- 당사자 간 토론이 격해질 때
- 일정 메시지마다(예: 10개의 메시지마다)
- 당사자 중 한 명이 요청할 때
