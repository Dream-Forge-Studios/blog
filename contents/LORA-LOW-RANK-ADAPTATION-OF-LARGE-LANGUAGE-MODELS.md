---
date: '2023-11-01'
title: 'LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS 논문 리뷰'
categories: ['Large Language']
summary: 'LORA 완벽 이해하기.'
thumbnail: './test.png'
---

<div id="ABSTRACT"></div>

# ABSTRACT

본 논문의 내용은 자연어 처리(NLP) 분야에서 사용되는 새로운 모델 최적화 기법인 'Low-Rank Adaptation, LoRA'에 관한 것입니다. <br><br>

이 방법은 대규모 언어 모델, 특히 GPT-3 같은 거대한 모델들을 특정 작업이나 도메인에 맞게 조정하는 과정에 관련되어 있습니다. <br><br>

자연어 처리에서는 일반적으로 대규모 데이터를 사용하여 모델을 사전 훈련하고, 특정 작업이나 도메인에 맞게 모델을 조정하는 과정을 거칩니다.

이러한 조정 과정은 몇 가지 특성을 가지고 있으며, 이는 대규모 모델의 특성과 관련이 있습니다.

<div id="모델의 크기와 조정 문제"></div>

## 모델의 크기와 조정 문제

GPT-3와 같은 대규모 모델들은 매우 크기 때문에, 모든 매개변수를 재훈련하는 전체 미세 조정은 비용이 많이 들고 실현 가능하지 않습니다. <br><br>

예를 들어, 1750억 개의 매개변수를 갖는 GPT-3 모델을 각각의 작업에 맞게 독립적으로 조정하는 것은 비용적으로 매우 부담스럽습니다.

<div id="LoRA의 개념"></div>

## LoRA의 개념

LoRA는 이러한 문제를 해결하기 위해 제안되었습니다. <br><br>

이 방법은 사전 훈련된 모델의 가중치를 고정하고, 트랜스포머 아키텍처의 각 계층에 rank decomposition matrices(순위 분해 행렬)을 삽입하여 훈련 가능한 매개변수의 수를 대폭 줄입니다. <br><br>

*rank decomposition matrices: 큰 행렬을 더 작은 행렬들의 곱으로 분해하는 것 <br>

m x n 크기의 행렬을 m x r과 m x n 크기로 분해하면 원래 m x n의 매겨변수가 m x r + m x n으로 줄어듭니다.

<div id="효율성과 성능"></div>

## 효율성과 성능

LoRA는 전체 미세 조정에 비해 훈련 가능한 매개변수 수를 1만 배나 줄이고, GPU 메모리 요구량도 3배 감소시킬 수 있습니다.

<div id="INTRODUCTION"></div>

# INTRODUCTION

<div id="fine-tuning"></div>

## fine-tuning

자연어 처리(NLP) 분야에서 대규모 사전 훈련된 언어 모델을 다양한 하위 작업에 맞게 조정하는 과정을 **fine-tuning**이라고 부릅니다.<br><br>

**fine-tuning**의 주요 문제점은 새로 조정된 모델이 원래 모델과 동일한 수의 많은 매개변수를 가지게 된다는 것입니다. <br><br>

예를 들어, GPT-2나 RoBERTa Large와 같은 모델들의 경우 이러한 문제가 "불편함" 수준이었지만, GPT-3와 같은 1750억 개의 훈련 가능한 매개변수를 가진 거대한 모델에서는 이 문제가 실제적인 배포와 운영상의 큰 도전으로 변합니다. <br><br>

따라서, 대규모 언어 모델을 효율적으로 다양한 작업에 맞게 조정하는 것은 NLP 분야에서 중요한 연구 주제가 되었습니다.

<div id="기존 fine-tuning 방식의 문제점"></div>

## 기존 fine-tuning 방식의 문제점

1. 추론 지연: 일부 기술들은 모델의 깊이를 늘림으로써 추론(inference) 시의 지연을 초래합니다. 즉, 모델이 새로운 데이터에 대한 결과를 생성하는 데 더 오래 걸리게 됩니다.

2. 사용 가능한 시퀀스 길이 감소: 다른 기술들은 모델이 처리할 수 있는 시퀀스(연속된 데이터의 길이)의 길이를 줄입니다. 이는 모델이 더 짧은 텍스트만 처리할 수 있게 만들어, 그 활용 범위를 제한합니다.

3. 모델 품질과 효율성의 균형 문제: 중요한 것은 이러한 방법들이 종종 전통적인 미세 조정 방법의 기준에 미치지 못한다는 것입니다. 즉, 모델의 효율성을 높이려는 시도가 때때로 모델의 품질을 저하시키는 결과를 초래합니다.

결론적으로, 이러한 접근 방식들은 운영 효율성을 높이기 위한 시도임에도 불구하고, 추론 속도 저하, 처리 가능한 데이터 길이 감소, 모델 품질 저하와 같은 문제들을 내포하고 있습니다. <br><br>

따라서, 이 분야의 연구자들은 모델의 효율성과 품질 사이에 균형을 맞추기 위한 더 나은 방법을 모색하고 있습니다. 

<div id="intrinsic rank"></div>

## intrinsic rank

intrinsic rank란 행렬이나 데이터 세트가 상대적으로 낮은 차원의 구조를 가지고 있음을 나타냅니다. <br><br>

예를 들어, 많은 수의 변수나 차원을 가진 데이터도 실제로는 몇 개의 주요 변수에 의해 주로 설명될 수 있을 때, 이 데이터는 intrinsic rank라고 할 수 있습니다.

Li et al. (2018a) 및 Aghajanyan et al. (2020)의 연구에서는 대규모 모델이 사실상 저차원의 내재적 특성을 가지고 있다는 것을 보여줍니다. <br><br>

즉, 모델이 많은 매개변수를 가지고 있지만, 실제로 모델이 학습하는 동안 실제로 중요한 가중치의 변화가 전체 가능한 변화 공간 중 매우 제한된 부분에서 발생한다는 것을 의미합니다. <br><br>

이러한 발견은 모델 학습과 최적화에 중요한 시사점을 줍니다. 전체 가중치를 직접 조정하는 대신, 중요한 가중치의 변화에 집중함으로써 학습을 더 효율적으로 만들 수 있습니다. 이는 더 적은 계산 자원을 사용하면서도 모델의 중요한 특성을 유지하거나 향상시킬 수 있게 해줍니다. 

<div id="LoRA의 접근 방식"></div>

## LoRA의 접근 방식

LoRA에서는 학습 과정에서 발생하는 가중치의 변화를 직접 조정하는 대신, 기존 가중치 행렬을 rank decomposition로 표현합니다. 

즉, 가중치 변화가 저차원의 구조를 가진다는 가정 하에, 이러한 구조를 효율적으로 학습하고 조정하는 것입니다.


가중치 변화의 저랭크 특성: 연구자들은 모델을 특정 작업에 맞게 조정할 때 가중치가 변하는 부분 역시 '저랭크(intrinsic rank)'라는 저차원의 특성을 가질 것이라고 가설을 세웠습니다. 이는 모델이 학습하는 동안 변하는 가중치들이 실제로는 매우 제한된 범위 내에서 변한다는 것을 의미합니다.

LoRA 접근 방식: LoRA는 신경망의 밀집층(dense layers)의 가중치 변화를 직접 최적화하는 대신, 이 가중치 변화의 순위 분해 행렬을 최적화하는 방식으로 작동합니다. 사전 훈련된 가중치는 고정된 상태로 유지되고, 밀집층의 변경만 조정됩니다.

저장 및 계산 효율성: GPT-3 175B 모델을 예로 들면, 전체 랭크(예: d)가 12,288이더라도, 매우 낮은 랭크(예: r)로도 충분하다는 것을 보여줍니다. 이는 r의 값이 1 또는 2 정도로 매우 낮을 수 있다는 것을 의미합니다. 이로 인해 LoRA는 저장 공간과 계산 효율성 측면에서 매우 효과적입니다.

<br>

이러한 개선을 통해 "Anchor DETR"는 학습 속도와 성능 모두에서 큰 향상을 보였습니다. 특히, 기존 DETR이 500 epoch 동안 학습해야 했던 성능을 "Anchor DETR"는 단 50 epoch 만에 달성하였습니다. 이는 학습 시간을 크게 줄여주는 중요한 개선점입니다.

<br>
<img style="width: 100%; margin-bottom: 40px;" id="output" src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FtGonQ%2Fbtr4RzSsCh6%2F6SZc1zIoRAiXqYNHeDjbc0%2Fimg.png">

"Anchor DETR"에서의 개선된 Object Query Design은 이미지 내의 특정 위치를 기준으로 하는 "anchor point"를 도입하여 해당 위치 주변을 학습하도록 설계되었습니다.

<br>
이를 통해 모델은 이미지 내의 객체를 더욱 효과적으로 탐지할 수 있게 됩니다. 그렇다면, 어떻게 해당 위치 주변을 학습하는 것일까요?

<div id="Anchor Point"></div>

# Anchor Point