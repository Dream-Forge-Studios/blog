---
date: '2023-11-01'
title: 'LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS 논문 리뷰 1'
categories: ['Large Language']
summary: 'LORA 완벽 이해하기.'
thumbnail: './test.png'
---

<div id="ABSTRACT"></div>

# ABSTRACT

본 논문의 내용은 자연어 처리(NLP) 분야에서 사용되는 새로운 모델 최적화 기법인 'Low-Rank Adaptation, LoRA'에 관한 것입니다. <br><br>

이 방법은 대규모 언어 모델, 특히 GPT-3 같은 거대한 모델들을 특정 작업이나 도메인에 맞게 조정하는 과정에 관련되어 있습니다. <br><br>

자연어 처리에서는 일반적으로 대규모 데이터를 사용하여 모델을 사전 훈련하고, 특정 작업이나 도메인에 맞게 모델을 조정하는 과정을 거칩니다.

이러한 조정 과정은 몇 가지 특성을 가지고 있으며, 이는 대규모 모델의 특성과 관련이 있습니다.

<div id="모델의 크기와 조정 문제"></div>

## 모델의 크기와 조정 문제

GPT-3와 같은 대규모 모델들은 매우 크기 때문에, 모든 매개변수를 재훈련하는 전체 미세 조정은 비용이 많이 들고 실현 가능하지 않습니다. <br><br>

예를 들어, 1750억 개의 매개변수를 갖는 GPT-3 모델을 각각의 작업에 맞게 독립적으로 조정하는 것은 비용적으로 매우 부담스럽습니다.

<div id="LoRA의 개념"></div>

## LoRA의 개념

LoRA는 이러한 문제를 해결하기 위해 제안되었습니다. <br><br>

이 방법은 사전 훈련된 모델의 가중치를 고정하고, 트랜스포머 아키텍처의 각 계층에 rank decomposition matrices(순위 분해 행렬)을 삽입하여 훈련 가능한 매개변수의 수를 대폭 줄입니다. <br><br>

*rank decomposition matrices: 큰 행렬을 더 작은 행렬들의 곱으로 분해하는 것 <br>

m x n 크기의 행렬을 m x r과 m x n 크기로 분해하면 원래 m x n의 매겨변수가 m x r + m x n으로 줄어듭니다.

<div id="효율성과 성능"></div>

## 효율성과 성능

LoRA는 전체 미세 조정에 비해 훈련 가능한 매개변수 수를 1만 배나 줄이고, GPU 메모리 요구량도 3배 감소시킬 수 있습니다.

<div id="INTRODUCTION"></div>

# INTRODUCTION

<div id="fine-tuning"></div>

## fine-tuning

자연어 처리(NLP) 분야에서 대규모 사전 훈련된 언어 모델을 다양한 하위 작업에 맞게 조정하는 과정을 **fine-tuning**이라고 부릅니다.<br><br>

**fine-tuning**의 주요 문제점은 새로 조정된 모델이 원래 모델과 동일한 수의 많은 매개변수를 가지게 된다는 것입니다. <br><br>

예를 들어, GPT-2나 RoBERTa Large와 같은 모델들의 경우 이러한 문제가 "불편함" 수준이었지만, GPT-3와 같은 1750억 개의 훈련 가능한 매개변수를 가진 거대한 모델에서는 이 문제가 실제적인 배포와 운영상의 큰 도전으로 변합니다. <br><br>

따라서, 대규모 언어 모델을 효율적으로 다양한 작업에 맞게 조정하는 것은 NLP 분야에서 중요한 연구 주제가 되었습니다.

<div id="기존 fine-tuning 방식의 문제점"></div>

## 기존 fine-tuning 방식의 문제점

1. **추론 지연**: 일부 기술들은 모델의 깊이를 늘림으로써 추론(inference) 시의 지연을 초래합니다. 즉, 모델이 새로운 데이터에 대한 결과를 생성하는 데 더 오래 걸리게 됩니다.

2. **사용 가능한 시퀀스 길이 감소**: 다른 기술들은 모델이 처리할 수 있는 시퀀스(연속된 데이터의 길이)의 길이를 줄입니다. 이는 모델이 더 짧은 텍스트만 처리할 수 있게 만들어, 그 활용 범위를 제한합니다.

3. **모델 품질과 효율성의 균형 문제**: 중요한 것은 이러한 방법들이 종종 전통적인 미세 조정 방법의 기준에 미치지 못한다는 것입니다. 즉, 모델의 효율성을 높이려는 시도가 때때로 모델의 품질을 저하시키는 결과를 초래합니다.

결론적으로, 이러한 접근 방식들은 운영 효율성을 높이기 위한 시도임에도 불구하고, 추론 속도 저하, 처리 가능한 데이터 길이 감소, 모델 품질 저하와 같은 문제들을 내포하고 있습니다. <br><br>

따라서, 이 분야의 연구자들은 모델의 효율성과 품질 사이에 균형을 맞추기 위한 더 나은 방법을 모색하고 있습니다. 

<div id="intrinsic rank"></div>

## intrinsic rank

intrinsic rank란 <u>행렬이나 데이터 세트가 상대적으로 낮은 차원의 구조</u>를 가지고 있음을 나타냅니다. <br><br>

예를 들어, 많은 수의 변수나 차원을 가진 데이터도 실제로는 몇 개의 주요 변수에 의해 주로 설명될 수 있을 때, 이 데이터는 intrinsic rank라고 할 수 있습니다.

Li et al. (2018a) 및 Aghajanyan et al. (2020)의 연구에서는 대규모 모델이 사실상 저차원의 내재적 특성을 가지고 있다는 것을 보여줍니다. <br><br>

즉, 모델이 많은 매개변수를 가지고 있지만, 실제로 모델이 학습하는 동안 실제로 중요한 가중치의 변화가 전체 가능한 변화 공간 중 매우 제한된 부분에서 발생한다는 것을 의미합니다. <br><br>

이러한 발견은 모델 학습과 최적화에 중요한 시사점을 줍니다. 전체 가중치를 직접 조정하는 대신, **중요한 가중치의 변화에 집중**함으로써 학습을 더 효율적으로 만들 수 있습니다. 이는 더 적은 계산 자원을 사용하면서도 모델의 중요한 특성을 유지하거나 향상시킬 수 있게 해줍니다. 

<div id="LoRA의 접근 방식"></div>

## LoRA의 접근 방식

LoRA에서는 학습 과정에서 발생하는 가중치의 변화를 직접 조정하는 대신, **기존 가중치 행렬을 rank decomposition로 표현**합니다. <br><br>

즉, 가중치 변화가 저차원의 구조를 가진다는 가정 하에, 이러한 구조를 효율적으로 학습하고 조정하는 것입니다. 이는 저장 공간과 계산 자원을 절약하면서도 모델의 성능을 유지하거나 향상시킬 수 있습니다.

<div id="LoRA의 주요 이점"></div>

## LoRA의 주요 이점

### 1. pretrained model 공유 및 작업 전환 효율성

LoRA는 하나의 프리트레인된(미리 학습된) 모델을 여러 작업에 재사용할 수 있게 합니다. <br><br>

이때, 모델의 대부분은 'freeze'하고, 특정 작업에 맞게 LoRA 모듈의 행렬을 'rank decomposition(A와 B로 바꿈)' 함으로써, 작업을 전환할 때의 저장 공간 요구량과 전환 오버헤드를 크게 줄일 수 있습니다. <br><br>

이는 특히 많은 작업들을 빠르고 효율적으로 처리해야 하는 환경에서 유용합니다.

### 2. 효율적인 훈련 및 하드웨어 요구 사항 감소

LoRA는 'adaptive optimizers(Adam, RNSprop 등)'를 사용할 때 훈련 과정을 더 효율적으로 만들고, 하드웨어에 대한 요구 사항을 최대 3배까지 줄일 수 있습니다. <br><br>

이는 모델의 대부분의 파라미터에 대해 기울기를 계산하거나 최적화기의 상태를 유지할 필요가 없기 때문입니다. 대신, **훨씬 작은 규모의 low-rank 행렬들만 최적화**하면 됩니다.

### 3. 배포 시 무지연

LoRA의 간단한 선형 디자인 덕분에, 훈련 가능한 행렬들을 동결된 가중치와 병합할 때, 완전히 미세조정된 모델과 비교하여 추론 지연이 전혀 없습니다. <br><br>

즉, **모델을 실제로 배포할 때 추가적인 시간 지연 없이 모델의 성능을 유지**할 수 있습니다.

### 4. 다양한 기존 방법론과의 조합 가능성

LoRA는 기존의 많은 방법들과 직교적(서로 영향을 주지 않는)이며, 예를 들어 prefix-tuning 같은 다른 방법들과 결합할 수 있습니다. <br><br>

이는 LoRA가 **유연하며 다양한 상황과 기존 기술들과 함께 사용될 수 있음**을 의미합니다.

<div id="PROBLEM STATEMENT"></div>

# PROBLEM STATEMENT

LoRA는 언어 모델링을 중심으로 하되, 다양한 학습 목표와 작업에 유연하게 적용될 수 있는 방법론을 제안하고 있습니다. <br><br>

이 방법론은 특히 **주어진 prompt에 대해 가장 적절한 텍스트를 생성하는 데 초점**을 맞추고 있습니다.<br><br>

*사전 학습된 자동 회귀 언어 모델 $P_{Φ}(y|x)$ <br><br>

기존에는 $\underset{Φ}{max} \sum_{(x,y)∈Z}^{} \sum_{t=1}^{|y|} log(P_Φ(y_t|x,y<t))$식과 같이 조건부 언어 모델링 목표를 최대화하기 위해 반복적으로 전체 매개변수를 업데이트(∆Φ)하는 과정이였습니다.<br><br>


이 매개변수의 크기(|∆Φ|)는 사전 학습된 모델의 매개변수 크기($|Φ_0|$)와 동일합니다. 따라서, 사전 학습된 모델이 매우 큰 경우(예: GPT-3의 경우 $|Φ_0|$ ≈ 1750억), 여러 개의 미세조정된 모델을 저장하고 배포하는 것은 매우 어려울 수 있습니다. <br><br>

LoRA는 $\underset{Φ}{max} \sum_{(x,y)∈Z}^{} \sum_{t=1}^{|y|} log(P_{Φ_0+∆Φ(\Theta )}(y_t|x,y<t))$식과 같이 <u>훨씬 작은 크기의 매개변수(low-rank) 세트(Θ)를 사용하여 특정 작업에 맞는 매개변수 증분(∆Φ = ∆Φ(Θ))을 인코딩</u>하는 더 효율적인 방법을 채택합니다. <br><br>

즉, ∆Φ를 찾는 과정이 Θ를 최적화하는 것으로 변환됩니다. 이를 통해, 학습해야 할 매개변수의 수(|Θ|)를 사전 학습된 모델의 매개변수 수($|Φ_0|$)의 0.01%로 줄일 수 있습니다.

<div id="OUR METHOD"></div>

# OUR METHOD

<div id="LOW-RANK-PARAMETRIZED UPDATE MATRICES"></div>

## LOW-RANK-PARAMETRIZED UPDATE MATRICES

Aghajanyan et al. (2020)에 따르면, 사전 학습된 언어 모델은 낮은 "내재 차원(intrinsic dimension)"을 가지고 있으며, 더 작은 부공간으로의 무작위 투영에도 불구하고 여전히 효율적으로 학습할 수 있습니다.<br><br>

이에 영감을 받아 연구자들은 적응 과정 중 가중치 업데이트도 낮은 "내재 랭크"(intrinsic rank)를 가진다고 가설을 세웠습니다. <br><br>

사전 학습된 가중치 행렬 $W_0 ∈ R^{d×k}$에 대해, 그 업데이트를 row-rank decomposition하여 <br><br>

$W_0 + ∆W = W_0 + BA$로 표현합니다. 여기서 $B ∈ R^{d×r}, A ∈ R^{r×k}$이며, $r$은 $min(d, k)$보다 작거나 같습니다.

### reparametrization

<img style="width: 50%; margin-bottom: 40px;" id="output" src="https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fdjg3BY%2FbtsgbIW06Oy%2F84lBLwcH7L50AK3omXiy51%2Fimg.png">

1. A와 B의 초기화: 

그림과 같이 행렬 A는 무작위 **가우시안(Gaussian) 분포**로 초기화되고, B는 **0으로 초기화**됩니다. 이렇게 하면, 훈련 시작 시 ∆W = BA의 값이 0이 됩니다.

2. $∆W$의 스케일링

$∆W$을 $\frac{\alpha}{r}$로 스케일링 합니다. $\alpha$는 상수로 스케일링 비율을 결정하는 값이고, r은 앞서 언급된 rank를 의미합니다. <br><br>

Yang & Hu (2021)의 연구에 따라, 연구팀은 α를 처음 시도한 r 값으로 설정하고, 이후에는 조정하지 않습니다. <br><br>

이는 학습 과정에서 일관된 스케일링을 유지하며, 모델이 다양한 크기의 r에 대해서도 안정적으로 학습할 수 있게 하여 하이퍼파라미터를 다시 조정할 필요가 적어지게 만듭니다.

### A Generalization of Full Fine-tuning


LoRA를 모든 가중치 행렬에 적용하고 모든 biases을 훈련할 때, LoRA의 r를 사전 학습된 가중치 행렬의 크기와 동일하게 설정하면 전체 미세조정의 표현력을 대략적으로 회복할 수 있습니다.<br><br>

이는 가능한 매개변수의 수를 늘릴수록, LoRA 훈련은 원래 모델을 훈련하는 것과 유사해지는 것을 의미합니다. 하지만 이전 방법들은 MLP에 수렴(adapter-based methods)하고 긴 입력 시퀀스를 처리하지 못하는 모델에 수렴( prefix-based
methods)합니다.<br><br>

결론적으로, LoRA는 <u>사전 학습된 모델의 가중치 행렬을 전랭크로 업데이트할 필요 없이 효율적으로 미세조정할 수 있는 방법</u>을 제공합니다. <br><br>

이는 전체 미세조정의 표현력을 유지하면서도, 다른 방법들에 비해 더 유연하고 효율적인 학습을 가능하게 합니다.

### No Additional Inference Latency

LoRA를 적용한 모델을 실제 환경에서 사용할 때, 연구팀은 $W = W_0 + BA$를 명시적으로 계산하고 저장합니다. <br><br>

여기서 $W$는 최종적으로 사용되는 가중치 행렬, $W_0$는 사전 학습된 가중치 행렬, $B$와 $A$는 LoRA를 통해 도입된 추가적인 행렬들입니다.<br><br>

또한 다른 하위 작업으로 전환하려면, 먼저 현재의 LoRA 업데이트($BA$)를 원래 가중치($W_0$)에서 제거하여 복원 후 새로운 하위 작업에 맞는 새로운 LoRA 업데이트($B_0A_0$)를 추가하기만 하면 됩니다. <br><br>

결론적으로, LoRA는 <u>모델의 가중치를 효율적으로 재조정하면서도, 실제 사용 시 추가적인 추론 지연 없이 빠른 성능을 유지할 수 있도록 설계</u>되었습니다.

<div id="LOW-RANK-PARAMETRIZED UPDATE MATRICES"></div>

## APPLYING LORA TO TRANSFORMER

LoRA는 신경망의 어떤 가중치 행렬 부분집합에도 적용될 수 있으며, 이를 통해 훈련 가능한 매개변수의 수를 줄일 수 있습니다. <br><br>

본 논문에서는 트랜스포머 아키텍처에 적용할 때를 중점에 두고 설명하고 있습니다. <br><br>

트랜스포머 아키텍처 내에는 self-attention 모듈에 네 개의 가중치 행렬(Wq, Wk, Wv, Wo)과 다층 퍼셉트론(MLP) 모듈에 두 개의 가중치 행렬이 있습니다.

### self-attention 가중치 행렬의 처리

트랜스포머 아키텍처의 self-attention 모듈은 Wq는 '쿼리(query)', Wk는 '키(key)', Wv는 '값(value)'에 해당하는 dmodel × dmodel 차원의 가중치 행렬을 가집니다.<br><br>

이는 Multi-Head Attention을 통해 head마다 차원을 나누어서 각 head가 입력 데이터의 다른 특성에 주의를 기울게 만듭니다. 이러한 다양한 관점들이 결합되어, 모델은 입력 데이터에 대한 더 풍부하고 복합적인 이해를 갖게 됩니다.<br><br>

반면, LoRA에서는 <u>Multi-Head로 차원을 나누는 대신, 전체 dmodel × dmodel 차원의 가중치 행렬을 사용</u>하며, LOW-RANK 수정을 통해 행렬의 일부분만을 수정하여 모델의 적응성을 높입니다.

### MLP 모듈의 처리

LoRA 적용은 self-attention 가중치의 적응에만 국한되며, MLP 모듈은 고정(freeze)됩니다. 

즉, 하위 작업에서 MLP 모듈은 훈련되지 않습니다. 이는 단순성과 매개변수 효율성을 위한 결정입니다.

### Practical Benefits and Limitations

### 이점

1. 메모리 및 저장 공간 사용 감소

- 이 기술은 특히 메모리 사용량을 크게 줄입니다. 예를 들어, Adam 옵티마이저로 훈련된 대형 트랜스포머 모델에서 VRAM 사용량을 최대 2/3까지 줄일 수 있습니다. <br><br>
- 이는 모델의 고정된 파라미터에 대해 최적화 상태를 저장할 필요가 없기 때문입니다. <br><br>
- GPT-3 175B 모델의 경우, 훈련 중 VRAM 소비량을 1.2TB에서 350GB로 줄였습니다.

2. 체크포인트 크기 감소

- 예를 들어, 'r = 4'로 설정하고 쿼리 및 값 프로젝션 매트릭스만 조정하는 경우, 체크포인트 크기를 약 10,000배 (350GB에서 35MB) 줄일 수 있습니다. <br><br>
- 이는 훨씬 적은 GPU로 훈련할 수 있게 하며, I/O 병목 현상을 피할 수 있습니다.

3. 태스크 간 빠른 전환

- 배포된 상태에서 다양한 태스크 간에 전환하는 비용을 크게 줄일 수 있습니다. <br><br>
- 이는 모든 파라미터를 바꾸는 대신 LoRA 가중치만 바꿔서 가능합니다.  <br><br>
- 결과적으로, 사전 훈련된 가중치를 VRAM에 저장한 기계에서 쉽게 맞춤형 모델을 교체하며 사용할 수 있습니다.

4. 훈련 속도 향상:

- GPT-3 175B의 경우, 전체 미세 조정에 비해 훈련 시 25%의 속도 향상을 관찰할 수 있습니다. <br><br>
- 이는 모든 파라미터에 대한 그래디언트 계산이 필요 없기 때문입니다.

### 한계점: 다양한 태스크 처리

- LoRA는 추가된 A와 B 가중치를 원래 모델의 가중치(W)에 통합하면, 모델은 한 번에 하나의 태스크만 효율적으로 처리할 수 있게 됩니다.<br><br>
- 지연 시간이 크게 중요하지 않은 상황에서는 다른 접근 방식을 사용할 수 있습니다. 각 batch 내의 sampl)에 대해 적합한 LoRA 모듈(즉, 적절한 A와 B 가중치 세트)을 동적으로 선택합니다.<br><br>
- 이렇게 하면 모델은 한 번의 forward pass에서 여러 태스크를 동시에 처리할 수 있지만, 이는 추가적인 추론 지연을 초래할 수 있습니다.

<div id="UNDERSTANDING THE LOW-RANK UPDATES"></div>

# UNDERSTANDING THE LOW-RANK UPDATES

### TRANSFORMER의 어떤 가중치에 LORA를 적용해야할까?

본 논문 연구팀은 GPT-3 175B 모델에 대해 총 18M(약 35MB, FP16 형식으로 저장된 경우)의 파라미터 예산을 설정했습니다.<br><br>

조정할 파라미터는 모델이 수행할 작업의 특성에 따라 결정됩니다. 예를 들어, 텍스트 분류는 마지막 층(분류층), 기계 번역은 인코더와 디코더 모듈에 초점을 맞춥니다. 

<img style="width: 100%; margin-bottom: 40px; margin-top: 40px;" id="output" src="https://miro.medium.com/v2/resize:fit:1400/1*PSquNw46U4bwr8XL9YqafA.png">

위의 표는 GPT-3 모델에 LoRA를 다양한 유형의 attention 가중치에 적용하고, 이후 WikiSQL과 MultiNLI 데이터셋에 대한 모델의 검증 정확도를 측정했습니다. <br><br>

연구에서는 모든 파라미터를 ∆Wq(쿼리의 변화 가중치)나 ∆Wk(키의 변화 가중치)에만 넣었을 때, 모델의 성능이 상당히 낮아진다는 것을 발견했습니다. 즉, 쿼리나 키 중 하나만 조정하는 것은 모델 성능에 좋지 않은 영향을 미칩니다. <br><br>

반면에, Wq(쿼리 가중치)와 Wv(밸류 가중치)를 모두 적응(조정)할 때 가장 좋은 결과를 얻었습니다. 이는 모델이 <u>쿼리와 밸류의 정보를 모두 활용하는 것이 중요하다는 것을 의미</u>합니다. <br><br>

rank가 4인 경우(낮은 rank)에도 ∆W가 충분한 정보를 포착한다는 것을 시사합니다. 즉, <u>rank를 높이는 것보다 다양한 유형의 가중치 행렬에 적응을 적용하는 것이 더 바람직</u>합니다. <br><br>

### LORA 최적의 RANK r은?

해당 논문에서는 대형 언어 모델의 업데이트를 위한 행렬인 <u>ΔW가 매우 작은 "intrinsic rank"</u>를 가질 수 있다는 것을 제안합니다. 

<img style="width: 100%; margin-bottom: 40px; margin-top: 40px;" id="output" src="https://miro.medium.com/v2/resize:fit:1400/1*nN8ig3MxGronbLkAzEcUvQ.png">

기존의 방법에서는 ΔW 행렬의 차원을 높게 설정하여 모델의 학습 속도와 메모리 사용량이 증가하는 문제가 있었습니다. 하지만 이번 연구에서는 ΔW 행렬의 차원을 낮게 설정해도 충분한 성능을 발휘할 수 있다는 것을 보여줍니다. <br><br>

하지만, 모든 작업이 작은 r이 충분한 성능을 발휘하다는 점도 고려해야합니다. <br><br>

예를 들어, 사전 훈련된 모델을 다른 언어로 되어 있는 데이터로 fine tuning할 때는 작은 r을 사용하는 것 보다 큰 r을 사용하는 것이 좀 더 뛰어난 성능을 발휘합니다.

### ΔW와 W는 어떤 관계를 가질까?