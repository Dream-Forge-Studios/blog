---
date: '2023-02-14'
title: '인공지능 필수 기초 2탄'
categories: ['Basic']
summary: '인공지능에 대한 필수 기초에 대해 총정리 하였습니다.'
thumbnail: './test.png'
---

<div id="8. Back propagation"></div>

# 8. Back propagation
> chain rule를 활용하여 미분을 하여 update할 값을 구한다.

---

<div id="9. activation funtion"></div>

# 9. activation funtion
> 인공 신경망에서 입력신호의 총합을 출력 신호로 변환해주는 함수

1. 이 함수는 주로 비선형 함수를 사용하며, 다음 층으로 신호를 전달하기 전에 비선형성을 추가하는 역할

2. 모델의 표현력(representational power)을 증가시키는 데 중요한 역할

3. 비선형 함수를 사용하지 않으면 다층 신경망(multi-layer neural network)의 표현력이 선형 함수의 결합으로 제한되어 복잡한 데이터 패턴을 학습할 수 없다.
    - why?
<img style="width: 100%;" src="https://adatis.co.uk/wp-content/uploads/historic/HughFreestone_clip_image012.jpg">
      <div style="display: flex; margin-top: 30px; margin-left: 18px">
      <div style="margin-top: 18px">퍼셉트론이 위의 그림과 같을 때 linear activation을 사용하여 전개해보면</div>
      </div>
   <img style="width: 300px; margin-right: 20px; margin-left: 18px; margin-top: 20px; margin-bottom: 0px;" id="output" src="https://latex.codecogs.com/svg.image?((xw_1+b_1)w_2+b_2)w_3+b_3">
   <img style="width: 340px; margin-right: 20px; margin-left: 18px; margin-top: 20px; margin-bottom: 0px;" id="output" src="https://latex.codecogs.com/svg.image?=(xw_1w_2+bw_2+b_2)w_3+b_3">
   <img style="width: 400px; margin-right: 20px; margin-left: 18px; margin-top: 20px; margin-bottom: 0px;" id="output" src="https://latex.codecogs.com/svg.image?=xw_1w_2w_3+b_1w_2w_3+b_2w_3+b_3">
          <div style="display: flex; margin-top: 30px; margin-left: 18px">
      <div style="margin-top: 18px">가 나오는데</div>
      </div>
    
   <div style="display: flex; margin-top: 30px; margin-left: 18px">
   <img style="width: 160px; margin-right: 20px; margin-left: 0px; margin-top: 8px; margin-bottom: 0px;" id="output" src="https://latex.codecogs.com/svg.image?w=w_1w_2w_3,">
   <img style="width: 260px; margin-right: 8px; margin-left: 0px; margin-top: 8px; margin-bottom: 0px;" id="output" src="https://latex.codecogs.com/svg.image?b=bw_2w_3+b_2w_3+b_3">
      <div style="margin-top: 18px">로 치환하면</div>
   </div>

   <div style="display: flex; margin-top: 10px; margin-left: 18px">
   <img style="width: 80px; margin-right: 8px; margin-left: 0px; margin-top: 8px; margin-bottom: 0px;" id="output" src="https://latex.codecogs.com/svg.image?aw+b">
   <div style="margin-top: 18px">가 나오기 때문에 표현력이 선형 함수의 결합으로 제한된다.</div>
   </div>

<div id="sigmoid"></div>

## sigmoid
<img style="width: 100%;" src="https://mblogthumb-phinf.pstatic.net/MjAyMDA3MDdfMTgx/MDAxNTk0MTMwODg2NzAw.Bgt42rm3pV0xTPfuVjN1UbXw9HchDcAdLdvnsrAQvJ0g.ILAv2yJkoMXNiWHKAUe0QswJWyr84GwwlRbXwxCogKUg.PNG.zzoyou_/sigmoid.png?type=w800">

1. 전구간 미분 가능

2. 좀 더 부드러운 분류 가능

3. 확률로 해석 가능

### sigmoid 이용한 이진 분류

- 고양이 강아지 분류의 예
   - 예측한 강아지 사진은 <img style="width: 50px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;" id="output" src="https://latex.codecogs.com/svg.image?q=1">, 예측한 고양이 사진은 <img style="width: 50px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;" id="output" src="https://latex.codecogs.com/svg.image?q=0">로 출력값이 나오고 정답은  <img style="width: 12px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;" id="output" src="https://latex.codecogs.com/svg.image?y">로 정의
    즉, <img style="width: 140px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;" id="output" src="https://latex.codecogs.com/svg.image?q^y(1-q)^{1-y}">를 키우면 된다.
   - 데이터를 넣는 것은 독립시행임으로 곱한 확률을 키우면 되는데 0~1사이의 값을 계속 곱하면 작아지게 된다.
    <div style="display: flex;">
    <div style="margin-top: 3px;">그러므로 log를 취해서</div> 
    <img style="width: 280px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;" id="output" src="https://latex.codecogs.com/svg.image?L=-\sum_n log(q_n^{y_n}(1-q_n)^{y_n}))">
      <div style="margin-top: 3px;">로 나타낸다.</div> 
  </div>
    <br>
    *마이너스를 붙이는 이유 : 줄여야 하는 것이 Loss 함수 이므로

    *로그를 취해도 상관없는 이유는 단조증가(monotonically increasing) - 줄이려는 방향이 같다.
  
    - logistic regression이라고 부름
      <div style="display: flex;">
      <div style="margin-top: 6px;">logit을 linear regression 한 것 (logit이란 log-odds = </div> <img style="width: 100px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;" id="output" src="https://latex.codecogs.com/svg.image?log( \frac{q}{1-q})"><div style="margin-top: 6px;">)</div>
          </div>
      <br>
      logit에서 q(확률)를 구하기 위해 역함수인 sigmoid를 통과시킨 것
      
      (logit에서 q를 구하는 식으로 바꾸면 sigmoid 식과 같음) 
  
  
<div id="log-likehood가 MSE 보다 나은 이유"></div>

## log-likehood가 MSE 보다 나은 이유

MSE : <img style="width: 80px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;" id="output" src="https://latex.codecogs.com/svg.image?(q-1)^2">을 minimize

log-likehood : <img style="width: 66px; margin-right: 6px; margin-left: 8px; margin-top: 10px; margin-bottom: 0px;" id="output" src="https://latex.codecogs.com/svg.image?-log q">을 minimize

1. 그래프를 그려보면 log-likehood 더 민감
    
   <img style="width: 50%; margin-right: 6px; margin-left: 8px; margin-top: 20px; margin-bottom: 20px;" id="output" src="test8Img/likehood.PNG">
   
2. mse는 non-convex, log-likehood는 convex일 확률이 높다.

<div id="딥러닝의 뿌리 이론"></div>

## 딥러닝의 뿌리 이론
> MLE(Maximum Likelihood Estimation)

*<img style="width: 140px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;" id="output" src="https://latex.codecogs.com/svg.image?p(y_i|f_w(x_i))">을 최대로 하는 것


1. mse : 가우시안 분포로 likelihood를 가정한 다음, <img style="width: 80px; margin-right: 6px; margin-left: 8px; margin-top: 16px; margin-bottom: 0px;" id="output" src="https://latex.codecogs.com/svg.image?f_w(x_i)">의 출력을 평균값 <img style="width: 14px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;" id="output" src="https://latex.codecogs.com/svg.image?\hat{y}">로 삼고 NLL 식 
2. Cross-Entropy : 베르누이 분포로 로 likelihood를 가정한 다음, <img style="width: 80px; margin-right: 6px; margin-left: 8px; margin-top: 16px; margin-bottom: 0px;" id="output" src="https://latex.codecogs.com/svg.image?f_w(x_i)">의 출력을 확률 <img style="width: 14px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;" id="output" src="https://latex.codecogs.com/svg.image?q">로 삼고 NLL식 (다중 분류에서는 multinoulli(Categorical) 분포)

*다중분류(softmax regression)  <img style="width: 120px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;" id="output" src="https://latex.codecogs.com/svg.image?q_1^{y_1}q_2^{y_2}q_3^{y_3}\cdot\cdot\cdot">(y는 [1,0,0], [0,1,0], [0,0,1])

*NLL : negative log-likelihood