{"componentChunkName":"component---src-templates-post-template-tsx","path":"/FlashAttention Fast and Memory Efficient Exact Attention with IO Awareness/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"html":"<div id=\"ABSTRACT\"></div>\n<h1>ABSTRACT</h1>\n<p>전통적인 트랜스포머 모델들은 긴 시퀀스를 처리할 때 많은 시간과 메모리를 필요로 합니다.</p>\n<br>\n<p>이는 self-attention 메커니즘의 시간과 메모리 복잡도가 시퀀스 길이에 따라 제곱으로 증가하기 때문입니다.</p>\n<br>\n<p>FlashAttention은 GPU 메모리 계층 사이의 읽기/쓰기를 고려하는 IO-aware attention 알고리즘으로, <u>트랜스포머 모델의 처리 속도를 향상시키기 위해 개발</u>되었습니다.</p>\n<br>\n<p>이 방법은 <u>tiling을 사용하여 GPU의 고대역폭 메모리(HBM)와 온칩 SRAM 사이의 메모리 읽기/쓰기 횟수를 줄입니다.</u></p>\n<br>\n<p>*tiling: 큰 데이터 세트나 계산 작업을 더 작고 관리하기 쉬운 부분(타일)으로 나누는 기술</p>\n<p>*HBM: GPU가 대량의 데이터를 저전력으로 빠르게 처리할 수 있게 함</p>\n<p>*SRAM: 캐시 메모리로 사용, DRAM보다 빠른 접근 속도</p>\n<br>\n<p>FlashAttention은 표준 주의력 알고리즘보다 적은 HBM 접근이 필요하며, 다양한 SRAM 크기에 대해 최적화되어 있습니다.</p>\n<br>\n<p>또한, FlashAttention은 block-sparse attention으로 확장되어, 기존의 어떤 approximate attention 방법보다 빠르게 작동합니다.</p>\n<p>*block-sparse attention: 입력 시퀀스의 특정 부분에 대한 attention을 계산</p>\n<br>\n<p>FlashAttention은 BERT-large, GPT-2, long-range arena 모델들의 훈련 속도를 대폭 향상시켰습니다.</p>\n<br>\n<p>예를 들어, BERT-large 모델에서는 기존의 훈련 속도 기록보다 15% 빠르고, GPT-2 모델에서는 3배, long-range arena에서는 2.4배의 속도 향상을 보였습니다.</p>\n<br>\n<p>이러한 향상된 속도 덕분에 FlashAttention은 더 긴 문맥을 가진 트랜스포머 모델을 가능하게 하여, 모델의 품질을 높입니다. (예: GPT-2에서의 낮은 혼란도(perplexity)와 긴 문서 분류에서의 성능 향상)</p>\n<br>\n<p>FlashAttention을 사용한 트랜스포머는 Path-X 챌린지(시퀀스 길이 16K)와 Path-256(시퀀스 길이 64K)에서 최초로 평균 이상의 성능을 달성했습니다.</p>\n<div id=\"Introduction\"></div>\n<h1>Introduction</h1>\n<p>트랜스포머의 핵심인  self-attention module이 시퀀스 길이에 따라 시간과 메모리 복잡도가 제곱으로 증가하기 때문에 긴 시퀀스에 대한 처리는 여전히 어려운 문제입니다.</p>\n<br>\n<p>많은 approximation attention 방법들(sparse approximation, low-rank approximation 등)이 주의력의 계산과 메모리 요구 사항을 줄이기 위해 개발되어 시퀀스 길이에 대해 거의 선형적인 계산 요구 사항을 가지게 하지만, 실제 처리 시간(wall-clock speedup)에서 개선을 보이지 않아 널리 채택되지 않았습니다.</p>\n<br>\n<p>이러한 접근법들의 주된 문제 중 하나는 FLOP(부동 소수점 연산의 수) 감소에 초점을 맞추는 것인데 메모리 접근 시간, 데이터 전송 시간, 다양한 하드웨어 자원의 효율적 사용 등은 모두 처리 시간에 영향을 미치지만 FLOP와는 직접적인 관련이 없습니다.</p>\n<br>\n<p>또한, 메모리 접근(IO)에서 발생하는 오버헤드를 무시하는 경향이 있습니다.</p>\n<img style=\"width: 100%; margin-bottom: 40px; margin-top: 40px;\" id=\"output\" src=\"https://raw.githubusercontent.com/hazyresearch/flash-attention/master/assets/flashattn_banner.jpg\">\n<p>본 논문에서는 attention 알고리즘을 개선하기 위한 새로운 접근법인 <strong>IO-aware</strong>을 제시합니다.</p>\n<br>\n<p><strong>IO-aware</strong>란 <u>메모리 접근(입출력, IO)을 세심하게 고려하는 것</u>을 말합니다. 구체적으로는, 다양한 종류의 메모리(빠른 메모리 SRAM와 비교적 느린 메모리 HBM) 사이에서 데이터를 읽고 쓰는 과정을 주의 깊게 관리하는 것입니다.</p>\n<br>\n<p>현대의 GPU에서는 계산 속도가 메모리 속도를 앞서고 있으며, 트랜스포머 모델에서 대부분의 연산은 메모리 접근에 의해 제한됩니다.</p>\n<br>\n<p>즉, <u>계산 자체보다는 데이터를 메모리에서 읽고 쓰는데 더 많은 시간이 소요</u>되는 경우가 많습니다.</p>\n<br>\n<p><strong>IO-aware</strong> 알고리즘은 데이터베이스 조인, 이미지 처리, 수치 선형 대수학 등 메모리 접근이 주요한 시간 소모 요소인 <u>메모리 제한 연산에서 중요한 역할</u>을 합니다.</p>\n<br>\n<p>그러나 현재 널리 사용되는 딥러닝 프레임워크인 파이썬의 PyTorch와 TensorFlow와 같은 인터페이스들은 메모리 접근을 세밀하게 제어할 수 있는 기능을 제공하지 않습니다.</p>\n<br>\n<p>그래서 고안된 FlashAttention의 주요 목표는 attention matrix를 HBM에 읽고 쓰는 것을 피하는 것입니다.</p>\n<br>\n<p>이를 위한 전략으로는</p>\n<div id=\"전략\"></div>\n<h2>전략</h2>\n<ol>\n<li>tiling: 입력을 블록으로 나누어 블록 내에서 여러번 softmax reduction를 수행한 후 결과를 통합합니다.</li>\n</ol>\n<p>*softmax reduction: 모델이 어떤 토큰에 더 많은 ‘주의’를 기울여야 하는지 계산하는 것</p>\n<ol start=\"2\">\n<li>\n<p>forward pass 과정에서 소프트맥스 정규화 인자를 SRAM에 저장하여, backward pass 과정에서 HBM에서 읽지 않고 SRAM에서 attention을 빠르게 재계산합니다.</p>\n <br>\n</li>\n<li>\n<p>CUDA에서 구현되어 메모리 접근을 세밀하게 제어할 수 있습니다.</p>\n<ul>\n<li>모든 attention 연산을 하나의 GPU 커널로 통합하여, 연산 효율성을 높입니다.</li>\n</ul>\n<p>*서로 다른 GPU 커널을 사용하면, 각 커널 간의 컨텍스트 전환(context switching)이 필요합니다.</p>\n</li>\n</ol>\n<p>이를 통한 구체적인 성능 개선으로는</p>\n<div id=\"성능 개선\"></div>\n<h2>성능 개선</h2>\n<ol>\n<li>\n<p>재계산으로 인한 FLOP 증가에도 불구하고, 표준 attention 대비 빠른 실행 속도(예: GPT-2에서 최대 7.6배 빠름)와 더 적은 메모리 사용(시퀀스 길이에 선형적)을 달성합니다.</p>\n <br>\n</li>\n<li>\n<p>IO complexity</p>\n<ul>\n<li>\n<p>IO complexity는 컴퓨터 알고리즘에서 데이터의 입출력(Input/Output) 작업이 얼마나 복잡한지를 나타내는 지표입니다.</p>\n<br>\n</li>\n<li>\n<p>FlashAttention은 표준 attention 대비 훨씬 적은 HBM 접근이 필요하여 IO complexity가 낮습니다.</p>\n<br>\n</li>\n<li>\n<p>FlashAttention의 HBM 접근 복잡도: <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><msup><mi>N</mi><mn>2</mn></msup><msup><mi>d</mi><mn>2</mn></msup><msup><mi>M</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O(N^2d^2M^{-1})</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0641em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">d</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">−</span><span class=\"mord mtight\">1</span></span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span><br>* <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>O</mi><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">O()</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">O</span><span class=\"mopen\">(</span><span class=\"mclose\">)</span></span></span></span></span>: 알고리즘의 성능 상한선</p>\n</li>\n<li>\n<p>표준 attention의 HBM 접근 복잡도: <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Ω</mi><mo stretchy=\"false\">(</mo><mi>N</mi><mi>d</mi><mo>+</mo><msup><mi>N</mi><mn>2</mn></msup><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">Ω(Nd+N^2)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">Ω</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"mord mathnormal\">d</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0641em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8141em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">2</span></span></span></span></span></span></span></span><span class=\"mclose\">)</span></span></span></span></span><br>* <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">Ω</mi><mo stretchy=\"false\">(</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">Ω()</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">Ω</span><span class=\"mopen\">(</span><span class=\"mclose\">)</span></span></span></span></span>: 알고리즘의 성능 하한선<br>* 시퀀스의 길이: <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>N</mi></mrow><annotation encoding=\"application/x-tex\">N</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">N</span></span></span></span></span>, SRAM 크기: <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>M</mi></mrow><annotation encoding=\"application/x-tex\">M</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.10903em;\">M</span></span></span></span></span>, attention head의 차원: <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>d</mi></mrow><annotation encoding=\"application/x-tex\">d</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6944em;\"></span><span class=\"mord mathnormal\">d</span></span></span></span></span></p>\n</li>\n</ul>\n</li>\n</ol>\n<p>FlashAttention의 확장 버전인 Block-Sparse FlashAttention이 있습니다.</p>\n<div id=\"다양한 확장 가능성\"></div>\n<h2>다양한 확장 가능성</h2>\n<p>본 논문에서는 다중 GPU에서의 주의력, 커널 회귀, 블록 희소 행렬 곱셈과 같은 다양한 연산에 FlashAttention을 확장할 수 있음을 논의합니다.</p>\n<br>\n<p><strong>Block-Sparse FlashAttention</strong></p>\n<br>\n<p>FlashAttention의 확장으로, Block-Sparse attention 계산을 수행합니다. 이는 특히 매우 긴 시퀀스 데이터를 처리할 때 유용합니다.</p>\n<br>\n<p>Block-Sparse attention는 모든 토큰 간이 아닌 특정 블록 내의 토큰들 끼리만 attention 계산을 하는 것을 의미합니다.</p>\n<div id=\"성능 평가 및 벤치마킹\"></div>\n<h2>성능 평가 및 벤치마킹</h2>\n<ol>\n<li>\n<p>모델 훈련 속도 향상: FlashAttention을 사용하면 트랜스포머 모델의 훈련이 기존 방법보다 더 빠르게 진행됩니다. 예를 들어, BERT-large, GPT-2, Long-range arena 모델들이 기존 대비 각각 15%, 3배, 2.4배 빠른 속도로 훈련됩니다.</p>\n</li>\n<li>\n<p>모델 품질 향상: FlashAttention은 더 긴 시퀀스를 처리할 수 있게 함으로써 모델의 품질을 향상시킵니다. GPT-2에서는 복잡도(perplexity)가 0.7 개선되었고, 긴 문서 분류 작업에서는 성능이 6.4 포인트 향상되었습니다.</p>\n</li>\n<li>\n<p>Path-X 및 Path-256 도전과제 수행: FlashAttention을 사용한 트랜스포머 모델은 Path-X 도전과제에서 처음으로 기회 수준 이상의 성능을 보였으며, Block-sparse FlashAttention을 사용한 모델은 더 긴 64K 시퀀스에서 Path-256 도전과제를 수행할 수 있었습니다.</p>\n</li>\n<li>\n<p>attention 벤치마킹: FlashAttention은 표준 attention 구현보다 최대 3배 빠르며, 128부터 2K까지의 일반적인 시퀀스 길이에서 더 나은 성능을 보입니다. 시퀀스 길이가 1K 이상인 경우, 일부 approximate attention methods (e.g., Linformer)가 더 빠르기 시작하지만, Block-sparse FlashAttention은 현재 알려진 모든 approximate attention methods보다 빠릅니다.</p>\n</li>\n</ol>","frontmatter":{"title":"FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness 논문 리뷰","summary":"FlashAttention 완벽 이해하기.","date":"2023.12.06.","categories":["LLM"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEZklEQVR42j2TCU8bVxDH9wNUvRRE1IQkrTh87PpcY0OIACGaqmoKSJFCA4ratFVDOJoAbShpEVcSEQg4YByccBgaKBgM5rC963O959tdr20gBkSq9qv0EaRKT6N5evrp/5+ZN0j151/gRo0Vx8pseJnNbMP1Zr3GgBah6nyLCRsbs8di8VA4Go8nGJbnOEEQRAAkUZRhRCoqq2zFeiuOXraZS63GElxvMWiMmArV5BebsOf28TiViERiFEWfwjwPIC8ACSZISemV8itWo05lxQ3FZoNRrzWgapNOhUHYiI2POyAWjsTiFE1DmAenMDiFTSa8orxMpy2wmPTwGHVaVF2IaQpObBsxh2Pyf5h5B5/IChKMMEdQFCsrK1UXfWrUo2YDZtBpMG2RujBfq8rHT5THw+FIMBiCPJVgGIZjT8zzLMsmEjSiRXUohkKdYrPOYkRNek1RwaWi/M8uXcgzYWr7mH17eydEkhRFwXJFIAABqvMCz8WiEaSgUGOx4MUmFLbKgKrOnc358P33Pv7og/qaamF7WiCXiIA/CYS0LO0qyTeZdHYvc/hmL7ub5hNxJCc3r6KiHJrMO/cJqtXU1V7Lzc2xGlS+lwNLjgHfVN/KrDOTlAHLSDwn8pzEswoQUiIPr8iZM2fP513EcUtHZ/uGz3twuLeyvDjy8M7wr9+vTo8+aPlpYqj/aD8jMicwJNl4TEhQgGFiIRIxmS0XLhY8HHgKW5g9SO1m0qzknbf/3Hnr676OHwf/6PK4XUf7u1AqKfApwAMmARg6Hg75N31IZWXVjMsVIYJPno5u7mylFMGx0vR7X03r9er6St3zwVY6sg3JjAwykrCXFCHPURSXoOhoGKmtrYuQ/sNdJSPxrlevJhc7+53XG1v1N25it78z9A7WhcitKOGPEH7YIeVEmYLOZY6TORZpbm4Ok8FEggGi4vP5Oga+bH9c9e09w602rKnb+EvvtUAgGCLDm+veTe9KIhKiwmQkGIiHSBqOqqfnt/U1D8tJHJBpGnTea+gaqW5o0TXexWpuFH5z2wp/JMXKKx7v0p/uDc/Sltez5V3bXl+DCdLa0rSxuhwORYGcTqWzronH3aNXq2vUZlxVbCv4qh4jAuubo22+sVbvutc9O/vaPTczNbkw8/KvBTdSWVG+uvya8O/EKfbg8B8yTtzts/3QdrXxZr21RFNaef5+c0N7jXlu6AHD8AQZDRDheffC2PCQffgJXAzLYF+P17M8NzMN32iWcb5sn50f8a77Rp4NdXfcmbA/o1hp/+BvJbXLC7Isp3leJoio68UUYrRchiJOx/j87PTc9DTgBZFYySip/ezx4dHb47f/Zg+OFSUtAjkJByKl4Eq922qJoQHS2fXIpNfdb2ueck5srHpIMsrxopLa298/lJNpUUrCvYUxmcyIkgITjoO/i4MlxKI04l5YLbfqK0pMj/p7XzgmXE7n/MIiw0IgJQBoUpEkBQA4V8BxIsuJ8ImiGJKIbG36/wPG3qQZ7mlVkwAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/story/static/3672896cbb49cb2a4050c9744a3e8496/d89b0/test.jpg","srcSet":"/story/static/3672896cbb49cb2a4050c9744a3e8496/aaa13/test.jpg 374w,\n/story/static/3672896cbb49cb2a4050c9744a3e8496/b3c6b/test.jpg 748w,\n/story/static/3672896cbb49cb2a4050c9744a3e8496/d89b0/test.jpg 1496w","sizes":"(min-width: 1496px) 1496px, 100vw"},"sources":[{"srcSet":"/story/static/3672896cbb49cb2a4050c9744a3e8496/0dfd6/test.webp 374w,\n/story/static/3672896cbb49cb2a4050c9744a3e8496/8acfa/test.webp 748w,\n/story/static/3672896cbb49cb2a4050c9744a3e8496/06565/test.webp 1496w","type":"image/webp","sizes":"(min-width: 1496px) 1496px, 100vw"}]},"width":1496,"height":1496}},"publicURL":"/story/static/3672896cbb49cb2a4050c9744a3e8496/test.png"}}}}]}},"pageContext":{"slug":"/FlashAttention Fast and Memory Efficient Exact Attention with IO Awareness/"}},"staticQueryHashes":[],"slicesMap":{}}