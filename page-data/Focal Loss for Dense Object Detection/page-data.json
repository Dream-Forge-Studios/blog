{"componentChunkName":"component---src-templates-post-template-tsx","path":"/Focal Loss for Dense Object Detection/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"html":"<p>본 논문은 One-stage detector의 학습 불균형이라는 단점을 해결하기 위한 내용이다.</p>\n<p>그렇다면 One-stage detector가 무엇일까요?</p>\n<div id=\"One-stage detector\"></div>\n<h1>One-stage detector</h1>\n<blockquote>\n<p>객체의 위치와 클래스를 동시에 한 단계에서 예측하는 방식입니다.</p>\n<p>이러한 방식의 주요 장점은 빠른 속도와 간단한 구조입니다.</p>\n</blockquote>\n<p>대표적인 예로는 YOLO(You Only Look Once)와 SSD(Single Shot MultiBox Detector)가 있습니다.</p>\n<div id=\"작동 원리\"></div>\n<h2>작동 원리</h2>\n<ol>\n<li>\n<p>입력 이미지</p>\n<ul>\n<li>일반적으로 전처리 단계를 거쳐 고정된 크기로 리사이징된 이미지가 네트워크의 입력으로 사용됩니다.</li>\n</ul>\n</li>\n<li>\n<p>특징 추출</p>\n<ul>\n<li>주어진 이미지에서 특징을 추출하기 위해 Convolutional Neural Network (CNN)이 사용됩니다. 이 CNN은 일반적으로 이미지 분류 문제에 대해 사전 훈련된 모델을 기반으로 합니다 (예: VGG16, ResNet 등).</li>\n</ul>\n</li>\n<li>\n<p>그리드 분할</p>\n<ul>\n<li>추출된 특징 맵을 그리드 셀로 나눕니다. 예를 들어, YOLOv1에서는 7x7 그리드를 사용했습니다.</li>\n</ul>\n</li>\n<li>\n<p>객체 예측</p>\n<ul>\n<li>각 그리드 셀에는 물체가 있을 확률(객체의 중심이 존재하는지), 바운딩 박스 정보(오프셋), 그리고 해당 물체의 클래스에 대한 확률 정보를 예측합니다.</li>\n</ul>\n</li>\n</ol>\n<img style=\"width: 100%; margin-bottom: 40px;\" id=\"output\" src=\"https://blog.nerdfactory.ai/assets/images/posts/2021-07-01-You-Only-Look-Once-YOLO/Untitled3.png\">\n*YOLOv1의 구조\n<ol start=\"5\">\n<li>\n<p>앵커 박스 사용</p>\n<ul>\n<li>SSD나 YOLOv2부터는 앵커 박스 또는 기본 박스(default boxes)를 사용하여 각 그리드 셀에서 여러 개의 바운딩 박스를 예측합니다. 앵커 박스는 다양한 크기와 비율을 가진 사전 정의된 박스로, 각 박스에 대해 물체가 있을 확률과 클래스 확률, 그리고 바운딩 박스 오프셋을 예측합니다.</li>\n</ul>\n</li>\n<li>\n<p>예측값 필터링:</p>\n<ul>\n<li>일반적으로 confidence threshold를 사용하여 낮은 확률을 가진 예측값을 필터링합니다.\n<ul>\n<li>confidence threshold\n<ul>\n<li>일정 수준 이상의 confidence score를 가진 예측만을 선택함으로써 결과의 수를 줄일 수 있습니다.</li>\n<li>Threshold를 너무 높게 설정하면 많은 객체를 놓칠 수 있으며, 너무 낮게 설정하면 잘못된 예측이 많아질 수 있습니다.</li>\n</ul>\n</li>\n</ul>\n</li>\n</ul>\n</li>\n<li>\n<p>비최대 억제 (NMS):</p>\n<ul>\n<li>여러 겹치는 박스 중에서 가장 확률이 높은 박스만을 선택하는 과정으로, 겹치는 박스들을 제거하여 최종 결과를 보다 명확하게 만듭니다.</li>\n</ul>\n</li>\n<li>\n<p>최종 객체 감지 결과:</p>\n<ul>\n<li>NMS를 거친 후, 최종적으로 선택된 바운딩 박스와 클래스 레이블이 최종 감지 결과로 반환됩니다.</li>\n</ul>\n</li>\n</ol>\n<p>그런데, 해당 방식은 학습 중 배경에 대한 바운딩 박스의 수가 실제 객체에 대한 바운딩 박스의 수보다 훨씬 많이 생성 됩니다.</p>\n<p>이는 학습 중에서 배경에 대한 박스를 출력하면 오류라고 학습이 되지만 그 빈도수가 너무 많다는 것이 학습에 방해가 된다는 뜻입니다. (easy negative)</p>\n<p>그래서 이러한 문제를 해결하기 위해 Focal Loss를 고안하였습니다.</p>\n<div id=\"Focal Loss\"></div>\n<h1>Focal Loss</h1>\n<p>배경에 해당하는 클래스가 주로 많기 때문에 이전에 활용되었던 Cross Entropy Loss는  학습 데이터에서 긍정적인 샘플(실제 객체)과 부정적인 샘플(배경) 간의 불균형이 발생합니다.</p>\n<blockquote>\n<p>Focal Loss는 이러한 문제를 해결하기 위해, 잘못 분류된 예제에 더 큰 가중치를 부여하고, 올바르게 분류된 예제에는 낮은 가중치를 부여하는 방식으로 작동합니다.</p>\n</blockquote>\n<img style=\"width: 100%; margin-bottom: 40px;\" id=\"output\" src=\"https://gaussian37.github.io/assets/img/dl/concept/focal_loss/1.png\">\n<ul>\n<li>\n<p>위 그래프는 γ=0일 때, Cross Entropy Loss와 같고 γ가 커질수록 잘못 분류 됬을 때 더 큰 가중치를 부여합니다.</p>\n</li>\n<li>\n<p>γ의 값이 5일 때 loss가 가장 빠르게 줄어드는 것을 볼 수 있습니다.</p>\n</li>\n<li>\n<p>Cross Entropy Loss의 잘 못 분류된 확률을 추가했고, γ의 따라 정도를 조절합니다.</p>\n</li>\n<li>\n<p>본 논문에서는 γ를 <strong>focusing parameter</strong>라고 부릅니다.</p>\n</li>\n</ul>","frontmatter":{"title":"Focal Loss for Dense Object Detection 논문 리뷰","summary":"Focal Loss for Dense Object Detection을 이해해보아요","date":"2020.07.29.","categories":["CV"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEZklEQVR42j2TCU8bVxDH9wNUvRRE1IQkrTh87PpcY0OIACGaqmoKSJFCA4ratFVDOJoAbShpEVcSEQg4YByccBgaKBgM5rC963O959tdr20gBkSq9qv0EaRKT6N5evrp/5+ZN0j151/gRo0Vx8pseJnNbMP1Zr3GgBah6nyLCRsbs8di8VA4Go8nGJbnOEEQRAAkUZRhRCoqq2zFeiuOXraZS63GElxvMWiMmArV5BebsOf28TiViERiFEWfwjwPIC8ACSZISemV8itWo05lxQ3FZoNRrzWgapNOhUHYiI2POyAWjsTiFE1DmAenMDiFTSa8orxMpy2wmPTwGHVaVF2IaQpObBsxh2Pyf5h5B5/IChKMMEdQFCsrK1UXfWrUo2YDZtBpMG2RujBfq8rHT5THw+FIMBiCPJVgGIZjT8zzLMsmEjSiRXUohkKdYrPOYkRNek1RwaWi/M8uXcgzYWr7mH17eydEkhRFwXJFIAABqvMCz8WiEaSgUGOx4MUmFLbKgKrOnc358P33Pv7og/qaamF7WiCXiIA/CYS0LO0qyTeZdHYvc/hmL7ub5hNxJCc3r6KiHJrMO/cJqtXU1V7Lzc2xGlS+lwNLjgHfVN/KrDOTlAHLSDwn8pzEswoQUiIPr8iZM2fP513EcUtHZ/uGz3twuLeyvDjy8M7wr9+vTo8+aPlpYqj/aD8jMicwJNl4TEhQgGFiIRIxmS0XLhY8HHgKW5g9SO1m0qzknbf/3Hnr676OHwf/6PK4XUf7u1AqKfApwAMmARg6Hg75N31IZWXVjMsVIYJPno5u7mylFMGx0vR7X03r9er6St3zwVY6sg3JjAwykrCXFCHPURSXoOhoGKmtrYuQ/sNdJSPxrlevJhc7+53XG1v1N25it78z9A7WhcitKOGPEH7YIeVEmYLOZY6TORZpbm4Ok8FEggGi4vP5Oga+bH9c9e09w602rKnb+EvvtUAgGCLDm+veTe9KIhKiwmQkGIiHSBqOqqfnt/U1D8tJHJBpGnTea+gaqW5o0TXexWpuFH5z2wp/JMXKKx7v0p/uDc/Sltez5V3bXl+DCdLa0rSxuhwORYGcTqWzronH3aNXq2vUZlxVbCv4qh4jAuubo22+sVbvutc9O/vaPTczNbkw8/KvBTdSWVG+uvya8O/EKfbg8B8yTtzts/3QdrXxZr21RFNaef5+c0N7jXlu6AHD8AQZDRDheffC2PCQffgJXAzLYF+P17M8NzMN32iWcb5sn50f8a77Rp4NdXfcmbA/o1hp/+BvJbXLC7Isp3leJoio68UUYrRchiJOx/j87PTc9DTgBZFYySip/ezx4dHb47f/Zg+OFSUtAjkJByKl4Eq922qJoQHS2fXIpNfdb2ueck5srHpIMsrxopLa298/lJNpUUrCvYUxmcyIkgITjoO/i4MlxKI04l5YLbfqK0pMj/p7XzgmXE7n/MIiw0IgJQBoUpEkBQA4V8BxIsuJ8ImiGJKIbG36/wPG3qQZ7mlVkwAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/story/static/3672896cbb49cb2a4050c9744a3e8496/d89b0/test.jpg","srcSet":"/story/static/3672896cbb49cb2a4050c9744a3e8496/aaa13/test.jpg 374w,\n/story/static/3672896cbb49cb2a4050c9744a3e8496/b3c6b/test.jpg 748w,\n/story/static/3672896cbb49cb2a4050c9744a3e8496/d89b0/test.jpg 1496w","sizes":"(min-width: 1496px) 1496px, 100vw"},"sources":[{"srcSet":"/story/static/3672896cbb49cb2a4050c9744a3e8496/0dfd6/test.webp 374w,\n/story/static/3672896cbb49cb2a4050c9744a3e8496/8acfa/test.webp 748w,\n/story/static/3672896cbb49cb2a4050c9744a3e8496/06565/test.webp 1496w","type":"image/webp","sizes":"(min-width: 1496px) 1496px, 100vw"}]},"width":1496,"height":1496}},"publicURL":"/story/static/3672896cbb49cb2a4050c9744a3e8496/test.png"}}}}]}},"pageContext":{"slug":"/Focal Loss for Dense Object Detection/"}},"staticQueryHashes":[],"slicesMap":{}}