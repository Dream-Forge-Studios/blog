{"componentChunkName":"component---src-templates-post-template-tsx","path":"/LORA LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"html":"<div id=\"ABSTRACT\"></div>\n<h1>ABSTRACT</h1>\n<p>본 논문의 내용은 새로운 모델 최적화 기법인 ‘Low-Rank Adaptation, LoRA’에 관한 것입니다. <br><br></p>\n<p>이 방법은 대규모 언어 모델, 특히 GPT-3 같은 거대한 모델들을 특정 작업이나 도메인에 맞게 조정하는 과정에 관련되어 있습니다. <br><br></p>\n<p>자연어 처리에서는 일반적으로 대규모 데이터를 사용하여 모델을 사전 훈련하고, 특정 작업이나 도메인에 맞게 모델을 조정하는 과정을 거칩니다.</p>\n<div id=\"모델의 크기와 조정 문제\"></div>\n<h2>모델의 크기와 조정 문제</h2>\n<p>GPT-3와 같은 대규모 모델들은 매우 크기 때문에, 모든 매개변수를 재훈련하는 전체 미세 조정은 비용이 많이 들고 실현 가능하지 않습니다. <br><br></p>\n<p>예를 들어, 1750억 개의 매개변수를 갖는 GPT-3 모델을 각각의 작업에 맞게 독립적으로 조정하는 것은 비용적으로 매우 부담스럽습니다.</p>\n<div id=\"LoRA의 개념\"></div>\n<h2>LoRA의 개념</h2>\n<p>LoRA는 이러한 문제를 해결하기 위해 제안되었습니다. <br><br></p>\n<p>이 방법은 사전 훈련된 모델의 가중치를 고정하고, 트랜스포머 아키텍처의 각 계층에 rank decomposition matrices(순위 분해 행렬)을 삽입하여 훈련 가능한 매개변수의 수를 대폭 줄입니다. <br><br></p>\n<p>*rank decomposition matrices: 큰 행렬을 더 작은 행렬들의 곱으로 분해하는 것 <br><br></p>\n<p>m x n 크기의 행렬을 m x r과 r x n 크기로 분해하면 원래 m x n의 매겨변수가 m x r + r x n으로 줄어듭니다.</p>\n<div id=\"효율성과 성능\"></div>\n<h2>효율성과 성능</h2>\n<p>LoRA는 전체 미세 조정에 비해 훈련 가능한 매개변수 수를 1만 배나 줄이고, GPU 메모리 요구량도 3배 감소시킬 수 있습니다.</p>\n<div id=\"INTRODUCTION\"></div>\n<h1>INTRODUCTION</h1>\n<div id=\"fine-tuning\"></div>\n<h2>fine-tuning</h2>\n<p>자연어 처리(NLP) 분야에서 대규모 사전 훈련된 언어 모델을 다양한 하위 작업에 맞게 조정하는 과정을 <strong>fine-tuning</strong>이라고 부릅니다.<br><br></p>\n<p><strong>fine-tuning</strong>의 주요 문제점은 새로 조정된 모델이 원래 모델과 동일한 수의 많은 매개변수를 가지게 된다는 것입니다. <br><br></p>\n<p>예를 들어, GPT-2나 RoBERTa Large와 같은 모델들의 경우 이러한 문제가 “불편함” 수준이었지만, GPT-3와 같은 1750억 개의 훈련 가능한 매개변수를 가진 거대한 모델에서는 이 문제가 실제적인 배포와 운영상의 큰 도전으로 변합니다. <br><br></p>\n<p>따라서, 대규모 언어 모델을 효율적으로 다양한 작업에 맞게 조정하는 것은 NLP 분야에서 중요한 연구 주제가 되었습니다.</p>\n<div id=\"기존 fine-tuning 방식의 문제점\"></div>\n<h2>기존 fine-tuning 방식의 문제점</h2>\n<ol>\n<li>\n<p><strong>추론 지연</strong>: 일부 기술들은 모델의 깊이를 늘림으로써 추론(inference) 시의 지연을 초래합니다. 즉, 모델이 새로운 데이터에 대한 결과를 생성하는 데 더 오래 걸리게 됩니다.</p>\n</li>\n<li>\n<p><strong>사용 가능한 시퀀스 길이 감소</strong>: 다른 기술들은 모델이 처리할 수 있는 시퀀스(연속된 데이터의 길이)의 길이를 줄입니다. 이는 모델이 더 짧은 텍스트만 처리할 수 있게 만들어, 그 활용 범위를 제한합니다.</p>\n</li>\n<li>\n<p><strong>모델 품질과 효율성의 균형 문제</strong>: 중요한 것은 이러한 방법들이 종종 전통적인 미세 조정 방법의 기준에 미치지 못한다는 것입니다. 즉, 모델의 효율성을 높이려는 시도가 때때로 모델의 품질을 저하시키는 결과를 초래합니다.</p>\n</li>\n</ol>\n<p>결론적으로, 이러한 접근 방식들은 운영 효율성을 높이기 위한 시도임에도 불구하고, 추론 속도 저하, 처리 가능한 데이터 길이 감소, 모델 품질 저하와 같은 문제들을 내포하고 있습니다. <br><br></p>\n<p>따라서, 이 분야의 연구자들은 모델의 효율성과 품질 사이에 균형을 맞추기 위한 더 나은 방법을 모색하고 있습니다.</p>\n<div id=\"intrinsic rank\"></div>\n<h2>intrinsic rank</h2>\n<p>intrinsic rank란 <u>행렬이나 데이터 세트가 상대적으로 낮은 차원의 구조</u>를 가지고 있음을 나타냅니다. <br><br></p>\n<p>예를 들어, 많은 수의 변수나 차원을 가진 데이터도 실제로는 몇 개의 주요 변수에 의해 주로 설명될 수 있을 때, 이 데이터는 intrinsic rank라고 할 수 있습니다.</p>\n<p>Li et al. (2018a) 및 Aghajanyan et al. (2020)의 연구에서는 대규모 모델이 사실상 저차원의 내재적 특성을 가지고 있다는 것을 보여줍니다. <br><br></p>\n<p>즉, 모델이 많은 매개변수를 가지고 있지만, 실제로 모델이 학습하는 동안 실제로 중요한 가중치의 변화가 전체 가능한 변화 공간 중 매우 제한된 부분에서 발생한다는 것을 의미합니다. <br><br></p>\n<p>이러한 발견은 모델 학습과 최적화에 중요한 시사점을 줍니다. 전체 가중치를 직접 조정하는 대신, <strong>중요한 가중치의 변화에 집중</strong>함으로써 학습을 더 효율적으로 만들 수 있습니다. 이는 더 적은 계산 자원을 사용하면서도 모델의 중요한 특성을 유지하거나 향상시킬 수 있게 해줍니다.</p>\n<div id=\"LoRA의 접근 방식\"></div>\n<h2>LoRA의 접근 방식</h2>\n<p>LoRA에서는 학습 과정에서 발생하는 가중치의 변화를 직접 조정하는 대신, <strong>기존 가중치 행렬을 rank decomposition로 표현</strong>합니다. <br><br></p>\n<p>즉, 가중치 변화가 저차원의 구조를 가진다는 가정 하에, 이러한 구조를 효율적으로 학습하고 조정하는 것입니다. 이는 저장 공간과 계산 자원을 절약하면서도 모델의 성능을 유지하거나 향상시킬 수 있습니다.</p>\n<div id=\"LoRA의 주요 이점\"></div>\n<h2>LoRA의 주요 이점</h2>\n<h3>1. pretrained model 공유 및 작업 전환 효율성</h3>\n<p>LoRA는 하나의 pretrained model 모델을 여러 작업에 재사용할 수 있게 합니다. <br><br></p>\n<p>이때, 모델의 대부분은 ‘freeze’하고, 특정 작업에 맞게 LoRA 모듈의 행렬을 ‘rank decomposition(A와 B로 바꿈)’ 함으로써, 작업을 전환할 때의 저장 공간 요구량과 전환 오버헤드를 크게 줄일 수 있습니다. <br><br></p>\n<p>이는 특히 많은 작업들을 빠르고 효율적으로 처리해야 하는 환경에서 유용합니다.</p>\n<h3>2. 효율적인 훈련 및 하드웨어 요구 사항 감소</h3>\n<p>LoRA는 ‘adaptive optimizers(Adam, RNSprop 등)‘를 사용할 때 훈련 과정을 더 효율적으로 만들고, 하드웨어에 대한 요구 사항을 최대 3배까지 줄일 수 있습니다. <br><br></p>\n<p>이는 모델의 대부분의 파라미터에 대해 기울기를 계산하거나 최적화기의 상태를 유지할 필요가 없기 때문입니다. 대신, <strong>훨씬 작은 규모의 low-rank 행렬들만 최적화</strong>하면 됩니다.</p>\n<h3>3. 배포 시 무지연</h3>\n<p>LoRA의 간단한 선형 디자인 덕분에, 훈련 가능한 행렬들을 동결된 가중치와 병합할 때, 완전히 미세조정된 모델과 비교하여 추론 지연이 전혀 없습니다. <br><br></p>\n<p>즉, <strong>모델을 실제로 배포할 때 추가적인 시간 지연 없이 모델의 성능을 유지</strong>할 수 있습니다.</p>\n<h3>4. 다양한 기존 방법론과의 조합 가능성</h3>\n<p>LoRA는 기존의 많은 방법들과 직교적(서로 영향을 주지 않는)이며, 예를 들어 prefix-tuning 같은 다른 방법들과 결합할 수 있습니다. <br><br></p>\n<p>이는 LoRA가 <strong>유연하며 다양한 상황과 기존 기술들과 함께 사용될 수 있음</strong>을 의미합니다.</p>\n<div id=\"PROBLEM STATEMENT\"></div>\n<h1>PROBLEM STATEMENT</h1>\n<p>LoRA는 언어 모델링을 중심으로 하되, 다양한 학습 목표와 작업에 유연하게 적용될 수 있는 방법론을 제안하고 있습니다. <br><br></p>\n<p>이 방법론은 특히 <strong>주어진 prompt에 대해 가장 적절한 텍스트를 생성하는 데 초점</strong>을 맞추고 있습니다.<br><br></p>\n<p>*사전 학습된 자동 회귀 언어 모델 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>P</mi><mi mathvariant=\"normal\">Φ</mi></msub><mo stretchy=\"false\">(</mo><mi>y</mi><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">P_{Φ}(y|x)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">Φ</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">x</span><span class=\"mclose\">)</span></span></span></span></span> <br><br></p>\n<p>기존에는 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi><munder><mo><mi>m</mi><mi>a</mi><mi>x</mi></mo><mi mathvariant=\"normal\">Φ</mi></munder></mi><msubsup><mo>∑</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo>∈</mo><mi>Z</mi></mrow><mrow></mrow></msubsup><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mi mathvariant=\"normal\">∣</mi></mrow></msubsup><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>P</mi><mi mathvariant=\"normal\">Φ</mi></msub><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>t</mi></msub><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo separator=\"true\">,</mo><msub><mi>y</mi><mrow><mo>&#x3C;</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\underset{Φ}{max} \\sum_{(x,y)∈Z}^{} \\sum_{t=1}^{|y|} log(P_Φ(y_t|x,y_{&#x3C;t}))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.7722em;vertical-align:-0.7443em;\"></span><span class=\"mord\"><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.4306em;\"><span style=\"top:-2.3557em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">Φ</span></span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span><span class=\"mop\"><span class=\"mord mathnormal\">ma</span><span class=\"mord mathnormal\">x</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7443em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:0em;\">∑</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.5029em;\"><span style=\"top:-2.4003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\">x</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose mtight\">)</span><span class=\"mrel mtight\">∈</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">Z</span></span></span></span><span style=\"top:-3.2029em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.4747em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:0em;\">∑</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0279em;\"><span style=\"top:-2.4003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.2029em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">∣</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord mtight\">∣</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2997em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3283em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">Φ</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mrel mtight\">&#x3C;</span><span class=\"mord mathnormal mtight\">t</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1774em;\"><span></span></span></span></span></span></span><span class=\"mclose\">))</span></span></span></span></span>식과 같이 조건부 언어 모델링 목표를 최대화하기 위해 반복적으로 전체 매개변수를 업데이트(∆Φ)하는 과정이였습니다.<br><br></p>\n<p>이 매개변수의 크기(|∆Φ|)는 사전 학습된 모델의 매개변수 크기(<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">∣</mi><msub><mi mathvariant=\"normal\">Φ</mi><mn>0</mn></msub><mi mathvariant=\"normal\">∣</mi></mrow><annotation encoding=\"application/x-tex\">|Φ_0|</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord\">Φ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span></span></span></span></span>)와 동일합니다. 따라서, 사전 학습된 모델이 매우 큰 경우(예: GPT-3의 경우 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">∣</mi><msub><mi mathvariant=\"normal\">Φ</mi><mn>0</mn></msub><mi mathvariant=\"normal\">∣</mi></mrow><annotation encoding=\"application/x-tex\">|Φ_0|</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord\">Φ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span></span></span></span></span> ≈ 1750억), 여러 개의 미세조정된 모델을 저장하고 배포하는 것은 매우 어려울 수 있습니다. <br><br></p>\n<p>LoRA는 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi><munder><mo><mi>m</mi><mi>a</mi><mi>x</mi></mo><mi mathvariant=\"normal\">Φ</mi></munder></mi><msubsup><mo>∑</mo><mrow><mo stretchy=\"false\">(</mo><mi>x</mi><mo separator=\"true\">,</mo><mi>y</mi><mo stretchy=\"false\">)</mo><mo>∈</mo><mi>Z</mi></mrow><mrow></mrow></msubsup><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mrow><mi mathvariant=\"normal\">∣</mi><mi>y</mi><mi mathvariant=\"normal\">∣</mi></mrow></msubsup><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy=\"false\">(</mo><msub><mi>P</mi><mrow><msub><mi mathvariant=\"normal\">Φ</mi><mn>0</mn></msub><mo>+</mo><mtext>∆</mtext><mi mathvariant=\"normal\">Φ</mi><mo stretchy=\"false\">(</mo><mi mathvariant=\"normal\">Θ</mi><mo stretchy=\"false\">)</mo></mrow></msub><mo stretchy=\"false\">(</mo><msub><mi>y</mi><mi>t</mi></msub><mi mathvariant=\"normal\">∣</mi><mi>x</mi><mo separator=\"true\">,</mo><msub><mi>y</mi><mrow><mo>&#x3C;</mo><mi>t</mi></mrow></msub><mo stretchy=\"false\">)</mo><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">\\underset{Φ}{max} \\sum_{(x,y)∈Z}^{} \\sum_{t=1}^{|y|} log(P_{Φ_0+∆Φ(\\Theta )}(y_t|x,y_{&#x3C;t}))</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.7722em;vertical-align:-0.7443em;\"></span><span class=\"mord\"><span class=\"mop op-limits\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.4306em;\"><span style=\"top:-2.3557em;margin-left:0em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">Φ</span></span></span></span><span style=\"top:-3em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span><span class=\"mop\"><span class=\"mord mathnormal\">ma</span><span class=\"mord mathnormal\">x</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.7443em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:0em;\">∑</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.5029em;\"><span style=\"top:-2.4003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mopen mtight\">(</span><span class=\"mord mathnormal mtight\">x</span><span class=\"mpunct mtight\">,</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">y</span><span class=\"mclose mtight\">)</span><span class=\"mrel mtight\">∈</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.07153em;\">Z</span></span></span></span><span style=\"top:-3.2029em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.4747em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mop\"><span class=\"mop op-symbol small-op\" style=\"position:relative;top:0em;\">∑</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:1.0279em;\"><span style=\"top:-2.4003em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">t</span><span class=\"mrel mtight\">=</span><span class=\"mord mtight\">1</span></span></span></span><span style=\"top:-3.2029em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">∣</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03588em;\">y</span><span class=\"mord mtight\">∣</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2997em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.01968em;\">l</span><span class=\"mord mathnormal\">o</span><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">g</span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">P</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3448em;\"><span style=\"top:-2.5198em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\"><span class=\"mord mtight\">Φ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3173em;\"><span style=\"top:-2.357em;margin-left:0em;margin-right:0.0714em;\"><span class=\"pstrut\" style=\"height:2.5em;\"></span><span class=\"sizing reset-size3 size1 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.143em;\"><span></span></span></span></span></span></span><span class=\"mbin mtight\">+</span><span class=\"mord mtight\">∆Φ</span><span class=\"mopen mtight\">(</span><span class=\"mord mtight\">Θ</span><span class=\"mclose mtight\">)</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3552em;\"><span></span></span></span></span></span></span><span class=\"mopen\">(</span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mathnormal mtight\">t</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span><span class=\"mord mathnormal\">x</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.03588em;\">y</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.2806em;\"><span style=\"top:-2.55em;margin-left:-0.0359em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mrel mtight\">&#x3C;</span><span class=\"mord mathnormal mtight\">t</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.1774em;\"><span></span></span></span></span></span></span><span class=\"mclose\">))</span></span></span></span></span>식과 같이 <u>훨씬 작은 크기의 매개변수(low-rank) 세트(Θ)를 사용하여 특정 작업에 맞는 매개변수 증분(∆Φ = ∆Φ(Θ))을 인코딩</u>하는 더 효율적인 방법을 채택합니다. <br><br></p>\n<p>즉, ∆Φ를 찾는 과정이 Θ를 최적화하는 것으로 변환됩니다. 이를 통해, 학습해야 할 매개변수의 수(|Θ|)를 사전 학습된 모델의 매개변수 수(<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi mathvariant=\"normal\">∣</mi><msub><mi mathvariant=\"normal\">Φ</mi><mn>0</mn></msub><mi mathvariant=\"normal\">∣</mi></mrow><annotation encoding=\"application/x-tex\">|Φ_0|</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord\">∣</span><span class=\"mord\"><span class=\"mord\">Φ</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\">∣</span></span></span></span></span>)의 0.01%로 줄일 수 있습니다.</p>\n<div id=\"OUR METHOD\"></div>\n<h1>OUR METHOD</h1>\n<div id=\"LOW-RANK-PARAMETRIZED UPDATE MATRICES\"></div>\n<h2>LOW-RANK-PARAMETRIZED UPDATE MATRICES</h2>\n<p>Aghajanyan et al. (2020)에 따르면, 사전 학습된 언어 모델은 낮은 “내재 차원(intrinsic dimension)“을 가지고 있으며, 더 작은 부공간으로의 무작위 투영에도 불구하고 여전히 효율적으로 학습할 수 있습니다.<br><br></p>\n<p>이에 영감을 받아 연구자들은 적응 과정 중 가중치 업데이트도 낮은 “내재 랭크”(intrinsic rank)를 가진다고 가설을 세웠습니다. <br><br></p>\n<p>사전 학습된 가중치 행렬 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>0</mn></msub><mo>∈</mo><msup><mi>R</mi><mrow><mi>d</mi><mo>×</mo><mi>k</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">W_0 ∈ R^{d×k}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8491em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8491em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"mbin mtight\">×</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span></span></span></span></span></span></span></span></span>에 대해, 그 업데이트를 row-rank decomposition하여 <br><br></p>\n<p><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>0</mn></msub><mo>+</mo><mtext>∆</mtext><mi>W</mi><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><mo>+</mo><mi>B</mi><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">W_0 + ∆W = W_0 + BA</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">∆</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span><span class=\"mord mathnormal\">A</span></span></span></span></span>로 표현합니다. 여기서 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>B</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>d</mi><mo>×</mo><mi>r</mi></mrow></msup><mo separator=\"true\">,</mo><mi>A</mi><mo>∈</mo><msup><mi>R</mi><mrow><mi>r</mi><mo>×</mo><mi>k</mi></mrow></msup></mrow><annotation encoding=\"application/x-tex\">B ∈ R^{d×r}, A ∈ R^{r×k}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.7224em;vertical-align:-0.0391em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:1.0435em;vertical-align:-0.1944em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8491em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\">d</span><span class=\"mbin mtight\">×</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">r</span></span></span></span></span></span></span></span></span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\">A</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">∈</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8491em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.00773em;\">R</span><span class=\"msupsub\"><span class=\"vlist-t\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.8491em;\"><span style=\"top:-3.063em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">r</span><span class=\"mbin mtight\">×</span><span class=\"mord mathnormal mtight\" style=\"margin-right:0.03148em;\">k</span></span></span></span></span></span></span></span></span></span></span></span></span>이며, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>r</mi></mrow><annotation encoding=\"application/x-tex\">r</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.02778em;\">r</span></span></span></span></span>은 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>m</mi><mi>i</mi><mi>n</mi><mo stretchy=\"false\">(</mo><mi>d</mi><mo separator=\"true\">,</mo><mi>k</mi><mo stretchy=\"false\">)</mo></mrow><annotation encoding=\"application/x-tex\">min(d, k)</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1em;vertical-align:-0.25em;\"></span><span class=\"mord mathnormal\">min</span><span class=\"mopen\">(</span><span class=\"mord mathnormal\">d</span><span class=\"mpunct\">,</span><span class=\"mspace\" style=\"margin-right:0.1667em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.03148em;\">k</span><span class=\"mclose\">)</span></span></span></span></span>보다 작거나 같습니다.</p>\n<h3>reparametrization</h3>\n<img style=\"width: 50%; margin-bottom: 40px;\" id=\"output\" src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fdjg3BY%2FbtsgbIW06Oy%2F84lBLwcH7L50AK3omXiy51%2Fimg.png\">\n<ol>\n<li>A와 B의 초기화:</li>\n</ol>\n<p>그림과 같이 행렬 A는 무작위 <strong>가우시안(Gaussian) 분포</strong>로 초기화되고, B는 <strong>0으로 초기화</strong>됩니다. 이렇게 하면, 훈련 시작 시 ∆W = BA의 값이 0이 됩니다.</p>\n<ol start=\"2\">\n<li><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>∆</mtext><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">∆W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">∆</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span>의 스케일링</li>\n</ol>\n<p><span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mtext>∆</mtext><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">∆W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord\">∆</span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span>을 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mfrac><mi>α</mi><mi>r</mi></mfrac></mrow><annotation encoding=\"application/x-tex\">\\frac{\\alpha}{r}</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:1.0404em;vertical-align:-0.345em;\"></span><span class=\"mord\"><span class=\"mopen nulldelimiter\"></span><span class=\"mfrac\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.6954em;\"><span style=\"top:-2.655em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.02778em;\">r</span></span></span></span><span style=\"top:-3.23em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"frac-line\" style=\"border-bottom-width:0.04em;\"></span></span><span style=\"top:-3.394em;\"><span class=\"pstrut\" style=\"height:3em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\"><span class=\"mord mathnormal mtight\" style=\"margin-right:0.0037em;\">α</span></span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.345em;\"><span></span></span></span></span></span><span class=\"mclose nulldelimiter\"></span></span></span></span></span></span>로 스케일링 합니다. <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>α</mi></mrow><annotation encoding=\"application/x-tex\">\\alpha</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.4306em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.0037em;\">α</span></span></span></span></span>는 상수로 스케일링 비율을 결정하는 값이고, r은 앞서 언급된 rank를 의미합니다. <br><br></p>\n<p>Yang &#x26; Hu (2021)의 연구에 따라, 연구팀은 α를 처음 시도한 r 값으로 설정하고, 이후에는 조정하지 않습니다. <br><br></p>\n<p>이는 학습 과정에서 일관된 스케일링을 유지하며, 모델이 다양한 크기의 r에 대해서도 안정적으로 학습할 수 있게 하여 하이퍼파라미터를 다시 조정할 필요가 적어지게 만듭니다.</p>\n<h3>A Generalization of Full Fine-tuning</h3>\n<p>LoRA를 모든 가중치 행렬에 적용하고 모든 biases을 훈련할 때, LoRA의 r를 사전 학습된 가중치 행렬의 크기와 동일하게 설정하면 전체 미세조정의 표현력을 대략적으로 회복할 수 있습니다.<br><br></p>\n<p>이는 가능한 매개변수의 수를 늘릴수록, LoRA 훈련은 원래 모델을 훈련하는 것과 유사해지는 것을 의미합니다. 하지만 이전 방법들은 MLP에 수렴(adapter-based methods)하고 긴 입력 시퀀스를 처리하지 못하는 모델에 수렴( prefix-based\r\nmethods)합니다.<br><br></p>\n<p>결론적으로, LoRA는 <u>사전 학습된 모델의 가중치 행렬을 전랭크로 업데이트할 필요 없이 효율적으로 미세조정할 수 있는 방법</u>을 제공합니다. <br><br></p>\n<p>이는 전체 미세조정의 표현력을 유지하면서도, 다른 방법들에 비해 더 유연하고 효율적인 학습을 가능하게 합니다.</p>\n<h3>No Additional Inference Latency</h3>\n<p>LoRA를 적용한 모델을 실제 환경에서 사용할 때, 연구팀은 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi><mo>=</mo><msub><mi>W</mi><mn>0</mn></msub><mo>+</mo><mi>B</mi><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">W = W_0 + BA</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span><span class=\"mrel\">=</span><span class=\"mspace\" style=\"margin-right:0.2778em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span><span class=\"mbin\">+</span><span class=\"mspace\" style=\"margin-right:0.2222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span><span class=\"mord mathnormal\">A</span></span></span></span></span>를 명시적으로 계산하고 저장합니다. <br><br></p>\n<p>여기서 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>W</mi></mrow><annotation encoding=\"application/x-tex\">W</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span></span></span></span></span>는 최종적으로 사용되는 가중치 행렬, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>는 사전 학습된 가중치 행렬, <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>B</mi></mrow><annotation encoding=\"application/x-tex\">B</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span></span></span></span></span>와 <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">A</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\">A</span></span></span></span></span>는 LoRA를 통해 도입된 추가적인 행렬들입니다.<br><br></p>\n<p>또한 다른 하위 작업으로 전환하려면, 먼저 현재의 LoRA 업데이트(<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mi>B</mi><mi>A</mi></mrow><annotation encoding=\"application/x-tex\">BA</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.6833em;\"></span><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span><span class=\"mord mathnormal\">A</span></span></span></span></span>)를 원래 가중치(<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>W</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">W_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.13889em;\">W</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.1389em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>)에서 제거하여 복원 후 새로운 하위 작업에 맞는 새로운 LoRA 업데이트(<span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><msub><mi>B</mi><mn>0</mn></msub><msub><mi>A</mi><mn>0</mn></msub></mrow><annotation encoding=\"application/x-tex\">B_0A_0</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.8333em;vertical-align:-0.15em;\"></span><span class=\"mord\"><span class=\"mord mathnormal\" style=\"margin-right:0.05017em;\">B</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:-0.0502em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span><span class=\"mord\"><span class=\"mord mathnormal\">A</span><span class=\"msupsub\"><span class=\"vlist-t vlist-t2\"><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.3011em;\"><span style=\"top:-2.55em;margin-left:0em;margin-right:0.05em;\"><span class=\"pstrut\" style=\"height:2.7em;\"></span><span class=\"sizing reset-size6 size3 mtight\"><span class=\"mord mtight\">0</span></span></span></span><span class=\"vlist-s\">​</span></span><span class=\"vlist-r\"><span class=\"vlist\" style=\"height:0.15em;\"><span></span></span></span></span></span></span></span></span></span></span>)를 추가하기만 하면 됩니다. <br><br></p>\n<p>결론적으로, LoRA는 <u>모델의 가중치를 효율적으로 재조정하면서도, 실제 사용 시 추가적인 추론 지연 없이 빠른 성능을 유지할 수 있도록 설계</u>되었습니다.</p>\n<div id=\"LOW-RANK-PARAMETRIZED UPDATE MATRICES\"></div>\n<h2>APPLYING LORA TO TRANSFORMER</h2>\n<p>LoRA는 신경망의 어떤 가중치 행렬 부분집합에도 적용될 수 있으며, 이를 통해 훈련 가능한 매개변수의 수를 줄일 수 있습니다. <br><br></p>\n<p>본 논문에서는 트랜스포머 아키텍처에 적용할 때를 중점에 두고 설명하고 있습니다. <br><br></p>\n<p>트랜스포머 아키텍처 내에는 self-attention 모듈에 네 개의 가중치 행렬(Wq, Wk, Wv, Wo)과 다층 퍼셉트론(MLP) 모듈에 두 개의 가중치 행렬이 있습니다.</p>\n<h3>self-attention 가중치 행렬의 처리</h3>\n<p>트랜스포머 아키텍처의 self-attention 모듈은 Wq는 ‘쿼리(query)’, Wk는 ‘키(key)’, Wv는 ‘값(value)‘에 해당하는 dmodel × dmodel 차원의 가중치 행렬을 가집니다.<br><br></p>\n<p>이는 Multi-Head Attention을 통해 head마다 차원을 나누어서 각 head가 입력 데이터의 다른 특성에 주의를 기울게 만듭니다. 이러한 다양한 관점들이 결합되어, 모델은 입력 데이터에 대한 더 풍부하고 복합적인 이해를 갖게 됩니다.<br><br></p>\n<p>반면, LoRA에서는 <u>Multi-Head로 차원을 나누는 대신, 전체 dmodel × dmodel 차원의 가중치 행렬을 사용</u>하며, LOW-RANK 수정을 통해 행렬의 일부분만을 수정하여 모델의 적응성을 높입니다.</p>\n<h3>MLP 모듈의 처리</h3>\n<p>LoRA 적용은 self-attention 가중치의 적응에만 국한되며, MLP 모듈은 고정(freeze)됩니다.</p>\n<p>즉, 하위 작업에서 MLP 모듈은 훈련되지 않습니다. 이는 단순성과 매개변수 효율성을 위한 결정입니다.</p>\n<h3>Practical Benefits and Limitations</h3>\n<h3>이점</h3>\n<ol>\n<li>메모리 및 저장 공간 사용 감소</li>\n</ol>\n<ul>\n<li>이 기술은 특히 메모리 사용량을 크게 줄입니다. 예를 들어, Adam 옵티마이저로 훈련된 대형 트랜스포머 모델에서 VRAM 사용량을 최대 2/3까지 줄일 수 있습니다. <br><br></li>\n<li>이는 모델의 고정된 파라미터에 대해 최적화 상태를 저장할 필요가 없기 때문입니다. <br><br></li>\n<li>GPT-3 175B 모델의 경우, 훈련 중 VRAM 소비량을 1.2TB에서 350GB로 줄였습니다.</li>\n</ul>\n<ol start=\"2\">\n<li>체크포인트 크기 감소</li>\n</ol>\n<ul>\n<li>예를 들어, ‘r = 4’로 설정하고 query 및 value 가중치 행렬만 조정하는 경우, 체크포인트 크기를 약 10,000배 (350GB에서 35MB) 줄일 수 있습니다. <br><br></li>\n<li>이는 훨씬 적은 GPU로 훈련할 수 있게 하며, I/O 병목 현상을 피할 수 있습니다.</li>\n</ul>\n<ol start=\"3\">\n<li>태스크 간 빠른 전환</li>\n</ol>\n<ul>\n<li>배포된 상태에서 다양한 태스크 간에 전환하는 비용을 크게 줄일 수 있습니다. <br><br></li>\n<li>이는 모든 파라미터를 바꾸는 대신 LoRA 가중치만 바꿔서 가능합니다.  <br><br></li>\n<li>결과적으로, 사전 훈련된 가중치를 VRAM에 저장한 기계에서 쉽게 맞춤형 모델을 교체하며 사용할 수 있습니다.</li>\n</ul>\n<ol start=\"4\">\n<li>훈련 속도 향상:</li>\n</ol>\n<ul>\n<li>GPT-3 175B의 경우, 전체 미세 조정에 비해 훈련 시 25%의 속도 향상을 관찰할 수 있습니다. <br><br></li>\n<li>이는 모든 파라미터에 대한 그래디언트 계산이 필요 없기 때문입니다.</li>\n</ul>\n<h3>한계점: 다양한 태스크 처리</h3>\n<ul>\n<li>LoRA는 추가된 A와 B 가중치를 원래 모델의 가중치(W)에 통합하면, 모델은 한 번에 하나의 태스크만 효율적으로 처리할 수 있게 됩니다.<br><br></li>\n<li>지연 시간이 크게 중요하지 않은 상황에서는 다른 접근 방식을 사용할 수 있습니다. 각 batch 내의 sampl)에 대해 적합한 LoRA 모듈(즉, 적절한 A와 B 가중치 세트)을 동적으로 선택합니다.<br><br></li>\n<li>이렇게 하면 모델은 한 번의 forward pass에서 여러 태스크를 동시에 처리할 수 있지만, 이는 추가적인 추론 지연을 초래할 수 있습니다.</li>\n</ul>\n<div id=\"UNDERSTANDING THE LOW-RANK UPDATES\"></div>\n<h1>UNDERSTANDING THE LOW-RANK UPDATES</h1>\n<h3>TRANSFORMER의 어떤 가중치에 LORA를 적용해야할까?</h3>\n<p>본 논문 연구팀은 GPT-3 175B 모델에 대해 총 18M(약 35MB, FP16 형식으로 저장된 경우)의 파라미터 예산을 설정했습니다.<br><br></p>\n<p>조정할 파라미터는 모델이 수행할 작업의 특성에 따라 결정됩니다. 예를 들어, 텍스트 분류는 마지막 층(분류층), 기계 번역은 인코더와 디코더 모듈에 초점을 맞춥니다.</p>\n<img style=\"width: 100%; margin-bottom: 40px; margin-top: 40px;\" id=\"output\" src=\"https://miro.medium.com/v2/resize:fit:1400/1*PSquNw46U4bwr8XL9YqafA.png\">\n<p>위의 표는 GPT-3 모델에 LoRA를 다양한 유형의 attention 가중치에 적용하고, 이후 WikiSQL과 MultiNLI 데이터셋에 대한 모델의 검증 정확도를 측정했습니다. <br><br></p>\n<p>연구에서는 모든 파라미터를 ∆Wq(쿼리의 변화 가중치)나 ∆Wk(키의 변화 가중치)에만 넣었을 때, 모델의 성능이 상당히 낮아진다는 것을 발견했습니다. 즉, 쿼리나 키 중 하나만 조정하는 것은 모델 성능에 좋지 않은 영향을 미칩니다. <br><br></p>\n<p>반면에, Wq(쿼리 가중치)와 Wv(밸류 가중치)를 모두 적응(조정)할 때 가장 좋은 결과를 얻었습니다. 이는 모델이 <u>쿼리와 밸류의 정보를 모두 활용하는 것이 중요하다는 것을 의미</u>합니다. <br><br></p>\n<p>rank가 4인 경우(낮은 rank)에도 ∆W가 충분한 정보를 포착한다는 것을 시사합니다. 즉, <u>rank를 높이는 것보다 다양한 유형의 가중치 행렬에 적응을 적용하는 것이 더 바람직</u>합니다. <br><br></p>\n<h3>LORA 최적의 RANK r은?</h3>\n<p>해당 논문에서는 대형 언어 모델의 업데이트를 위한 행렬인 <u>ΔW가 매우 작은 “intrinsic rank”</u>를 가질 수 있다는 것을 제안합니다.</p>\n<img style=\"width: 100%; margin-bottom: 40px; margin-top: 40px;\" id=\"output\" src=\"https://miro.medium.com/v2/resize:fit:1400/1*nN8ig3MxGronbLkAzEcUvQ.png\">\n<p>기존의 방법에서는 ΔW 행렬의 차원을 높게 설정하여 모델의 학습 속도와 메모리 사용량이 증가하는 문제가 있었습니다. 하지만 이번 연구에서는 ΔW 행렬의 차원을 낮게 설정해도 충분한 성능을 발휘할 수 있다는 것을 보여줍니다. <br><br></p>\n<p>하지만, 모든 작업이 작은 r이 충분한 성능을 발휘하다는 점도 고려해야합니다. <br><br></p>\n<p>예를 들어, 사전 훈련된 모델을 다른 언어로 되어 있는 데이터로 fine tuning할 때는 작은 r을 사용하는 것 보다 큰 r을 사용하는 것이 좀 더 뛰어난 성능을 발휘합니다.</p>\n<h3>ΔW와 W는 어떤 관계를 가질까?</h3>\n<p>ΔW는 랜덤 행렬보다 W와 더 강한 상관관계를 가지고 있습니다. 이는 ΔW가 W에 이미 존재하는 특징을 강조한다는 것을 의미합니다.</p>\n<img style=\"width: 70%; margin-bottom: 40px; margin-top: 40px;\" id=\"output\" src=\"/5c4b8ebdf50dc073db0166181e45ad87/compare.png\">\n<p>U와 V는 ΔW와 W의 관계를 조사하기 위해 singular 벡터를 이용하여 W를 ΔW의 r-차원 서브스페이스로 투영하는 left/right singular-vector matrix입니다. <br><br></p>\n<p>해당 식은 SVD(특이값 분해)와 관련 있으며 left singular-vector matrix는 데이터의 가장 중요한 특성을 포착하고, right singular-vector matrix 데이터 변환에 있어서 방향을 제공합니다. <br><br></p>\n<p>이를 통해, W의 singular 벡터가 ΔW의 singular 벡터와 얼마나 유사한지를 확인할 수 있습니다. <br><br></p>\n<p>그 다음, 투영된 W와 원래의 W의 Frobenius norm을 비교합니다. Frobenius norm은 행렬의 크기를 나타내는 지표입니다. <br><br></p>\n<h4>도표 해석</h4>\n<ol>\n<li>∆W와 W 사이의 강한 상관관계</li>\n</ol>\n<p>여기서 ∆W는 모델의 가중치 행렬 W에 대한 어떤 변화(업데이트)를 나타냅니다. ∆W가 W와 강한 상관관계를 가지고 있다는 것은, ∆W가 W에 이미 존재하는 특징들을 강조한다는 의미입니다.<br><br></p>\n<p>즉, 훈련 과정에서 모델이 기존에 가지고 있던 중요한 특성들을 더 강화하고 있다는 것입니다.</p>\n<ol start=\"2\">\n<li>W의 singular directions과 ∆W의 차이</li>\n</ol>\n<p>∆W는 W의 singular directions을 단순히 반복하는 것이 아니라, W에서 강조되지 않은 방향들을 강화합니다. <br><br></p>\n<p>이는 모델이 새로운 정보를 학습할 때 기존의 중요하지 않았던 방향을 더 주목하고, 그 방향을 강조하는 방식으로 발전한다는 것을 의미합니다.</p>\n<ol start=\"3\">\n<li>큰 증폭 계수(amplification factor)</li>\n</ol>\n<p>r이 4일 때 ∆W의 증폭 계수가 21.5로 매우 크다는 것이 관찰됩니다. 이는 특정 차원에서의 변화가 상당히 크다는 것을 나타냅니다.<br><br></p>\n<p>r이 64일 때는 작게 증폭 되는데, 이는 특정 작업에 대한 모델의 적응에서 중요한 특징들이 명확히 존재한다는 점을 의미합니다.</p>","frontmatter":{"title":"LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS 논문 리뷰","summary":"LORA 완벽 이해하기.","date":"2023.11.01.","categories":["LLM","Fine-Tuning"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEZklEQVR42j2TCU8bVxDH9wNUvRRE1IQkrTh87PpcY0OIACGaqmoKSJFCA4ratFVDOJoAbShpEVcSEQg4YByccBgaKBgM5rC963O959tdr20gBkSq9qv0EaRKT6N5evrp/5+ZN0j151/gRo0Vx8pseJnNbMP1Zr3GgBah6nyLCRsbs8di8VA4Go8nGJbnOEEQRAAkUZRhRCoqq2zFeiuOXraZS63GElxvMWiMmArV5BebsOf28TiViERiFEWfwjwPIC8ACSZISemV8itWo05lxQ3FZoNRrzWgapNOhUHYiI2POyAWjsTiFE1DmAenMDiFTSa8orxMpy2wmPTwGHVaVF2IaQpObBsxh2Pyf5h5B5/IChKMMEdQFCsrK1UXfWrUo2YDZtBpMG2RujBfq8rHT5THw+FIMBiCPJVgGIZjT8zzLMsmEjSiRXUohkKdYrPOYkRNek1RwaWi/M8uXcgzYWr7mH17eydEkhRFwXJFIAABqvMCz8WiEaSgUGOx4MUmFLbKgKrOnc358P33Pv7og/qaamF7WiCXiIA/CYS0LO0qyTeZdHYvc/hmL7ub5hNxJCc3r6KiHJrMO/cJqtXU1V7Lzc2xGlS+lwNLjgHfVN/KrDOTlAHLSDwn8pzEswoQUiIPr8iZM2fP513EcUtHZ/uGz3twuLeyvDjy8M7wr9+vTo8+aPlpYqj/aD8jMicwJNl4TEhQgGFiIRIxmS0XLhY8HHgKW5g9SO1m0qzknbf/3Hnr676OHwf/6PK4XUf7u1AqKfApwAMmARg6Hg75N31IZWXVjMsVIYJPno5u7mylFMGx0vR7X03r9er6St3zwVY6sg3JjAwykrCXFCHPURSXoOhoGKmtrYuQ/sNdJSPxrlevJhc7+53XG1v1N25it78z9A7WhcitKOGPEH7YIeVEmYLOZY6TORZpbm4Ok8FEggGi4vP5Oga+bH9c9e09w602rKnb+EvvtUAgGCLDm+veTe9KIhKiwmQkGIiHSBqOqqfnt/U1D8tJHJBpGnTea+gaqW5o0TXexWpuFH5z2wp/JMXKKx7v0p/uDc/Sltez5V3bXl+DCdLa0rSxuhwORYGcTqWzronH3aNXq2vUZlxVbCv4qh4jAuubo22+sVbvutc9O/vaPTczNbkw8/KvBTdSWVG+uvya8O/EKfbg8B8yTtzts/3QdrXxZr21RFNaef5+c0N7jXlu6AHD8AQZDRDheffC2PCQffgJXAzLYF+P17M8NzMN32iWcb5sn50f8a77Rp4NdXfcmbA/o1hp/+BvJbXLC7Isp3leJoio68UUYrRchiJOx/j87PTc9DTgBZFYySip/ezx4dHb47f/Zg+OFSUtAjkJByKl4Eq922qJoQHS2fXIpNfdb2ueck5srHpIMsrxopLa298/lJNpUUrCvYUxmcyIkgITjoO/i4MlxKI04l5YLbfqK0pMj/p7XzgmXE7n/MIiw0IgJQBoUpEkBQA4V8BxIsuJ8ImiGJKIbG36/wPG3qQZ7mlVkwAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/3672896cbb49cb2a4050c9744a3e8496/d89b0/test.jpg","srcSet":"/static/3672896cbb49cb2a4050c9744a3e8496/aaa13/test.jpg 374w,\n/static/3672896cbb49cb2a4050c9744a3e8496/b3c6b/test.jpg 748w,\n/static/3672896cbb49cb2a4050c9744a3e8496/d89b0/test.jpg 1496w","sizes":"(min-width: 1496px) 1496px, 100vw"},"sources":[{"srcSet":"/static/3672896cbb49cb2a4050c9744a3e8496/0dfd6/test.webp 374w,\n/static/3672896cbb49cb2a4050c9744a3e8496/8acfa/test.webp 748w,\n/static/3672896cbb49cb2a4050c9744a3e8496/06565/test.webp 1496w","type":"image/webp","sizes":"(min-width: 1496px) 1496px, 100vw"}]},"width":1496,"height":1496}},"publicURL":"/static/3672896cbb49cb2a4050c9744a3e8496/test.png"}}}}]}},"pageContext":{"slug":"/LORA LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS/"}},"staticQueryHashes":[],"slicesMap":{}}