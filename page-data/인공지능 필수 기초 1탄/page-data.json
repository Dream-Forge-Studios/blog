{"componentChunkName":"component---src-templates-post-template-tsx","path":"/인공지능 필수 기초 1탄/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"html":"<div id=\"1. weight와 bias\"></div>\n<h1>1. weight와 bias</h1>\n<div style=\"display: flex; margin-top: 0px\">\n<img style=\"width: 100%; margin-right: 8px; margin-left: 0px; margin-top: 8px; margin-bottom: 0px;\" id=\"output\" src=\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*upfpVueoUuKPkyX3PR3KBg.png\">\n</div>\n<blockquote>\n<p><u>weight</u>는 input의 정보를 얼마나 보낼건지(중요도), <u>bias</u>는 각 노드의 민감도를 정한다.</p>\n</blockquote>\n<hr>\n<div id=\"2. 선형회귀\"></div>\n<h1>2. 선형회귀</h1>\n<div style=\"display: flex; margin-top: 0px\">\n<div style=\"width: 50%;\"></div>\n<img style=\"width: 100%; margin-right: 8px; margin-left: 0px; margin-top: 8px; margin-bottom: 0px;\" id=\"output\" src=\"https://upload.wikimedia.org/wikipedia/commons/b/be/Normdist_regression.png\">\n<div style=\"width: 50%;\"></div>\n</div>\n<blockquote>\n<p>입력과 출력의 관계(함수)를 선형으로 추정하는 것</p>\n</blockquote>\n<p>즉, ax+b 함수에서 입력과 출력의 관계를 잘 설명하는 a,b를 구하는 것</p>\n<br>\r\n그렇다면 어떻게 a,b를 찾아내는 것인가?\n<div id=\"loss(cost) funtion\"></div>\n<h2>loss(cost) funtion</h2>\n<blockquote>\n<p>예측값과 실제값의 차이</p>\n</blockquote>\n<p>loss를 최소화하는 a,b가 바로 최적에 a,b이다.</p>\n<p>선형회귀에서는 mse(mean squared error - 실제값과 예측값 차이의 제곱의 평균)를 사용하여 loss를 구하는 것이 일반적이다.</p>\n<div style=\"display: flex; margin-top: 0px\">\n<div style=\"margin-top: 10px\">(</div>\n<img style=\"width: 300px; margin-right: 8px; margin-left: 10px; margin-top: 8px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?L = e_1^2 + e_2^2 + e_3^2 + e_4^2 + \\cdots + e_n^2 \">\n<div style=\"margin-top: 10px\">)</div>\n</div>\n<br>\r\n<br>\r\n그렇다면 loss를 최소화하는 a,b를 어떻게 찾아가야할까?\n<p>a,b를 일일이 바꿔가면서 하는 것일까? 이는 너무 비효율적이다.</p>\n<p>이제 loss를 최소화하는 다양한 방법들을 알아보자.</p>\n<hr>\n<div id=\"3. Gradient descent(경사하강법)\"></div>\n<h1>3. Gradient descent(경사하강법)</h1>\n<blockquote>\n<p>처음 랜덤으로 a,b를 정한 다음 모든 값에 대한 loss를 구한 후 <u>gradient(기울기)의 반대 방향</u>으로 이동</p>\n</blockquote>\n<div id=\"기울기의 반대 방향을 향해 이동하는 이유는?\"></div>\n<h2>기울기의 반대 방향을 향해 이동하는 이유는?</h2>\n<blockquote>\n<p>기울기가 0에 가까운 지점이 최적에 값이며 기울기는 가장 가파른 방향을 향하기 때문이다.</p>\n</blockquote>\n<p>하지만 계속 가파른 방향의 반대방향으로 향하다 보면 최적의 값을 지나버릴 수 있다.</p>\n<p>그래서 Learning rate가 필요하다.</p>\n<div id=\"Learning rate\"></div>\n<h2>Learning rate</h2>\n<blockquote>\n<p>얼마나 이동할 것인지를 조절하는 것, 기호로는 <img style=\"width: 16px; margin-right: 8px; margin-left: 4px; margin-top: 0px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?\\alpha\"></p>\n</blockquote>\n<div style=\"display: flex; margin-top: 0px\">\n<img style=\"width: 300px; margin-right: 8px; margin-left: 0px; margin-top: 8px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?\\begin{bmatrix}\na_{k+1} \\\\ b_{k+1}\n\\end{bmatrix} = \\begin{bmatrix}\na_{k} \\\\ b_{k}\n\\end{bmatrix} - \\alpha \\begin{bmatrix}\n\\frac{\\partial L}{\\partial a} \\\\ \\frac{\\partial L}{\\partial b}\n\\end{bmatrix}\">\n</div>\n<div id=\"Gradient descent의 단점\"></div>\n<h2>Gradient descent의 단점</h2>\n<ol>\n<li>\n<p>모든 값에 대한 loss를 고려해서 너무 오래 걸린다.</p>\n</li>\n<li>\n<p>local minimum</p>\n</li>\n</ol>\n<div style=\"display: flex; margin-top: -40px\">\r\n<div style=\"width: 50%;\"></div>\r\n<img style=\"width: 100%; margin-right: 8px; margin-left: 0px; margin-top: 8px; margin-bottom: 0px;\" id=\"output\" src=\"   https://upload.wikimedia.org/wikipedia/commons/thumb/6/68/Extrema_example_original.svg/440px-Extrema_example_original.svg.png\">\r\n<div style=\"width: 50%;\"></div>\r\n</div>\r\n<br>\n<p>이러한 단점을 해결하기 위한 방법은 어떤 것이 있을까?</p>\n<hr>\n<div id=\"4. SGD\"></div>\n<h1>4. SGD</h1>\n<blockquote>\n<p>랜덤하게 비복원추출로 데이터 하나씩 뽑아서 loss를 만듬</p>\n</blockquote>\n<div style=\"display: flex; margin-top: 0px\">\n<img style=\"width: 100%; margin-right: 8px; margin-left: 0px; margin-top: 8px; margin-bottom: 0px;\" id=\"output\" src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F99848D4C5B613C0C15\">\n</div>\n<div id=\"왜 gradient가 +를 향하지 않는가?\"></div>\n<h2>왜 gradient가 +를 향하지 않는가?</h2>\n<p>하나만 보고 방향을 결정하기 때문이다.</p>\n<p>하나만 보기 때문에 빠르다는 장점이 있다.</p>\n<p>또한 local minimum 으로부터 탈출의 기회가 되기도 한다.</p>\n<hr>\n<div id=\"5. mini-batch SGD\"></div>\n<h1>5. mini-batch SGD</h1>\n<blockquote>\n<p>데이터를 하나씩 뽑는 것이 아니라 batch size만큼 뽑아 비복원추출하여 loss를 만듬</p>\n</blockquote>\n<div style=\"display: flex; margin-top: 0px\">\n<img style=\"width: 100%; margin-right: 8px; margin-left: 0px; margin-top: 8px; margin-bottom: 0px;\" id=\"output\" src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Ft1.daumcdn.net%2Fcfile%2Ftistory%2F9987E53A5D0E1B7C2E\">\n</div>\n<br>\n<p>현재는 gpu가 병렬 연산을 가능하게 하므로 빠르게 계산할 수 있다.</p>\n<div id=\"hyperparameter\"></div>\n<h2>hyperparameter</h2>\n<blockquote>\n<p>정해줘야 하는 값</p>\n</blockquote>\n<ol>\n<li>\n<p>Epoch : 전체 데이터를 얼마나 반복할 것 인가</p>\n</li>\n<li>\n<p>Batch size : 몇 개씩 볼 것 인가</p>\n</li>\n<li>\n<p>Learning rate : 얼마큼 갈 것인가</p>\n</li>\n<li>\n<p>Initial weight</p>\n</li>\n<li>\n<p>model architecture (layer 수, node 수, activation 함수 등)</p>\n</li>\n<li>\n<p>loss 함수</p>\n</li>\n</ol>\n<ul>\n<li>적절한 값</li>\n</ul>\n<ol>\n<li>batch size 8k 까지만</li>\n</ol>\n<div style=\"display: flex; margin-top: 0px\">\n<img style=\"width: 100%; margin-right: 8px; margin-left: 0px; margin-top: 8px; margin-bottom: 0px;\" id=\"output\" src=\"https://mblogthumb-phinf.pstatic.net/MjAxODEyMjRfMjMg/MDAxNTQ1NjE5MzY1NDMx.N6wnv9T-HolFs8i310qG3mXeR0SKaAWoLEjWOokVAXog.S5lQjzsk5VUhlUAfvlMl_hsZkWeYaIzKSwYEdlW1fSEg.PNG.wideeyed/13.png?type=w800\">\n</div>\n<ol start=\"2\">\n<li>\n<p>batch size 두배하면 learning rate도 두배 (Linear Scaling Rule)</p>\n</li>\n<li>\n<p>warmup</p>\n</li>\n</ol>\n<div style=\"display: flex; margin-top: 0px\">\n<img style=\"width: 100%; margin-right: 8px; margin-left: 0px; margin-top: 8px; margin-bottom: 0px;\" id=\"output\" src=\"https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&amp;fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbTWPA7%2Fbtq1Ntn3oSX%2FWj1DImFnZHkLznhTeDPGu0%2Fimg.png\">\n</div>\n<ol start=\"4\">\n<li>\n<p>Weight initialization</p>\n<br>\r\nU() 균일 분포, N() 정규 분포, \n<p>N in 이전 레이어의 노드 수, N out 다음 레이어의 노드 수</p>\n</li>\n</ol>\n<ul>\n<li>LeCun</li>\n</ul>\n<div style=\"display: flex; margin-top: 6px; margin-left: 30px\">\n<img style=\"width: 200px; margin-right: 20px; margin-left: 10px; margin-top: 8px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?w\\sim U(-\\sqrt{\\frac{3}{N_{in}}}, \\sqrt{\\frac{3}{N_{in}}})\">\n<div style=\"margin-top: 18px\">or</div>\n<img style=\"width: 140px; margin-right: 8px; margin-left: 18px; margin-top: 8px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?w\\sim N(0, \\sqrt{\\frac{1}{N_{in}}})\">\n</div>\n<br>\n<ul>\n<li>Xavier (sigmoid/tanh 사용하는 신경망)</li>\n</ul>\n<div style=\"display: flex; margin-top: 10px; margin-left: 30px\">\n   <img style=\"width: 300px; margin-right: 20px; margin-left: 10px; margin-top: 8px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?w\\sim U(-\\sqrt{\\frac{6}{N_{in}+N_{out}}}, \\sqrt{\\frac{6}{N_{in}+N_{out}}})\">\n   <div style=\"margin-top: 16px\">or</div>\n   <img style=\"width: 200px; margin-right: 8px; margin-left: 18px; margin-top: 8px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?w\\sim N(0, \\sqrt{\\frac{2}{N_{in}+N_{out}}})\">\n   </div>\n   <br>\n<ul>\n<li>\n<p>He (ReLU 사용하는 신경망)</p>\n<div style=\"display: flex; margin-top: 30px; margin-left: 18px\">\n <img style=\"width: 200px; margin-right: 20px; margin-left: 10px; margin-top: 8px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?w\\sim U(-\\sqrt{\\frac{6}{N_{in}}}, \\sqrt{\\frac{6}{N_{in}}})\">\n <div style=\"margin-top: 18px\">or</div>\n <img style=\"width: 140px; margin-right: 8px; margin-left: 18px; margin-top: 8px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?w\\sim N(0, \\sqrt{\\frac{2}{N_{in}}})\">\n </div>\n</li>\n</ul>\n<hr>\n<div id=\"6. Adam\"></div>\n<h1>6. Adam</h1>\n<blockquote>\n<p>momentum과 RMSprop을 합쳐놓은 최적화 알고리즘</p>\n</blockquote>\n<div id=\"momentum과 RMSprop\"></div>\n<h2>momentum과 RMSprop</h2>\n<img style=\"width: 100%; margin-right: 8px; margin-left: 0px; margin-top: 50px; margin-bottom: 0px;\" id=\"output\" src=\"/story/865376140edf9605c777bad3930a4225/mom.png\">\n<img style=\"width: 100%; margin-right: 8px; margin-left: 0px; margin-top: 80px; margin-bottom: 0px;\" id=\"output\" src=\"/story/f80ed32bdf4b95e93be915954c9f89a6/rms.png\">\n<img style=\"width: 100%; margin-right: 8px; margin-left: 0px; margin-top: 80px; margin-bottom: 0px;\" id=\"output\" src=\"/story/0d4fa4950d155b36f15e923fe0107935/adam.png\">\n<hr>\n<div id=\"7. K-Fold Cross Validation\"></div>\n<h1>7. K-Fold Cross Validation</h1>\n<div id=\"Validation\"></div>\n<h2>Validation</h2>\n<ol>\n<li>\n<p>training 데이터 중 학습 도중 test할 때 쓰는 데이터</p>\n</li>\n<li>\n<p>하이퍼파라미터 선택을 위한 데이터</p>\n</li>\n</ol>\n<img style=\"width: 100%; margin-right: 8px; margin-left: 0px; margin-top: 50px; margin-bottom: 0px;\" id=\"output\" src=\"/story/45d6352ab15583aecdee061b495bc59c/kf.png\">","frontmatter":{"title":"인공지능 필수 기초 1탄","summary":"인공지능에 대한 필수 기초에 대해 총정리 하였습니다.","date":"2023.02.14.","categories":["AI BASIC"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEZklEQVR42j2TCU8bVxDH9wNUvRRE1IQkrTh87PpcY0OIACGaqmoKSJFCA4ratFVDOJoAbShpEVcSEQg4YByccBgaKBgM5rC963O959tdr20gBkSq9qv0EaRKT6N5evrp/5+ZN0j151/gRo0Vx8pseJnNbMP1Zr3GgBah6nyLCRsbs8di8VA4Go8nGJbnOEEQRAAkUZRhRCoqq2zFeiuOXraZS63GElxvMWiMmArV5BebsOf28TiViERiFEWfwjwPIC8ACSZISemV8itWo05lxQ3FZoNRrzWgapNOhUHYiI2POyAWjsTiFE1DmAenMDiFTSa8orxMpy2wmPTwGHVaVF2IaQpObBsxh2Pyf5h5B5/IChKMMEdQFCsrK1UXfWrUo2YDZtBpMG2RujBfq8rHT5THw+FIMBiCPJVgGIZjT8zzLMsmEjSiRXUohkKdYrPOYkRNek1RwaWi/M8uXcgzYWr7mH17eydEkhRFwXJFIAABqvMCz8WiEaSgUGOx4MUmFLbKgKrOnc358P33Pv7og/qaamF7WiCXiIA/CYS0LO0qyTeZdHYvc/hmL7ub5hNxJCc3r6KiHJrMO/cJqtXU1V7Lzc2xGlS+lwNLjgHfVN/KrDOTlAHLSDwn8pzEswoQUiIPr8iZM2fP513EcUtHZ/uGz3twuLeyvDjy8M7wr9+vTo8+aPlpYqj/aD8jMicwJNl4TEhQgGFiIRIxmS0XLhY8HHgKW5g9SO1m0qzknbf/3Hnr676OHwf/6PK4XUf7u1AqKfApwAMmARg6Hg75N31IZWXVjMsVIYJPno5u7mylFMGx0vR7X03r9er6St3zwVY6sg3JjAwykrCXFCHPURSXoOhoGKmtrYuQ/sNdJSPxrlevJhc7+53XG1v1N25it78z9A7WhcitKOGPEH7YIeVEmYLOZY6TORZpbm4Ok8FEggGi4vP5Oga+bH9c9e09w602rKnb+EvvtUAgGCLDm+veTe9KIhKiwmQkGIiHSBqOqqfnt/U1D8tJHJBpGnTea+gaqW5o0TXexWpuFH5z2wp/JMXKKx7v0p/uDc/Sltez5V3bXl+DCdLa0rSxuhwORYGcTqWzronH3aNXq2vUZlxVbCv4qh4jAuubo22+sVbvutc9O/vaPTczNbkw8/KvBTdSWVG+uvya8O/EKfbg8B8yTtzts/3QdrXxZr21RFNaef5+c0N7jXlu6AHD8AQZDRDheffC2PCQffgJXAzLYF+P17M8NzMN32iWcb5sn50f8a77Rp4NdXfcmbA/o1hp/+BvJbXLC7Isp3leJoio68UUYrRchiJOx/j87PTc9DTgBZFYySip/ezx4dHb47f/Zg+OFSUtAjkJByKl4Eq922qJoQHS2fXIpNfdb2ueck5srHpIMsrxopLa298/lJNpUUrCvYUxmcyIkgITjoO/i4MlxKI04l5YLbfqK0pMj/p7XzgmXE7n/MIiw0IgJQBoUpEkBQA4V8BxIsuJ8ImiGJKIbG36/wPG3qQZ7mlVkwAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/story/static/3672896cbb49cb2a4050c9744a3e8496/d89b0/test.jpg","srcSet":"/story/static/3672896cbb49cb2a4050c9744a3e8496/aaa13/test.jpg 374w,\n/story/static/3672896cbb49cb2a4050c9744a3e8496/b3c6b/test.jpg 748w,\n/story/static/3672896cbb49cb2a4050c9744a3e8496/d89b0/test.jpg 1496w","sizes":"(min-width: 1496px) 1496px, 100vw"},"sources":[{"srcSet":"/story/static/3672896cbb49cb2a4050c9744a3e8496/0dfd6/test.webp 374w,\n/story/static/3672896cbb49cb2a4050c9744a3e8496/8acfa/test.webp 748w,\n/story/static/3672896cbb49cb2a4050c9744a3e8496/06565/test.webp 1496w","type":"image/webp","sizes":"(min-width: 1496px) 1496px, 100vw"}]},"width":1496,"height":1496}},"publicURL":"/story/static/3672896cbb49cb2a4050c9744a3e8496/test.png"}}}}]}},"pageContext":{"slug":"/인공지능 필수 기초 1탄/"}},"staticQueryHashes":[],"slicesMap":{}}