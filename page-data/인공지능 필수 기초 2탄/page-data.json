{"componentChunkName":"component---src-templates-post-template-tsx","path":"/인공지능 필수 기초 2탄/","result":{"data":{"allMarkdownRemark":{"edges":[{"node":{"html":"<div id=\"8. Back propagation\"></div>\n<h1>8. Back propagation</h1>\n<blockquote>\n<p>chain rule를 활용하여 미분을 하여 update할 값을 구한다.</p>\n</blockquote>\n<hr>\n<div id=\"9. activation funtion\"></div>\n<h1>9. activation funtion</h1>\n<blockquote>\n<p>인공 신경망에서 입력신호의 총합을 출력 신호로 변환해주는 함수</p>\n</blockquote>\n<ol>\n<li>\n<p>이 함수는 주로 비선형 함수를 사용하며, 다음 층으로 신호를 전달하기 전에 비선형성을 추가하는 역할</p>\n</li>\n<li>\n<p>모델의 표현력(representational power)을 증가시키는 데 중요한 역할</p>\n</li>\n<li>\n<p>비선형 함수를 사용하지 않으면 다층 신경망(multi-layer neural network)의 표현력이 선형 함수의 결합으로 제한되어 복잡한 데이터 패턴을 학습할 수 없다.</p>\n<ul>\n<li>why?</li>\n</ul>\n</li>\n</ol>\n<img style=\"width: 100%;\" src=\"https://adatis.co.uk/wp-content/uploads/historic/HughFreestone_clip_image012.jpg\">\n      <div style=\"display: flex; margin-top: 30px; margin-left: 18px\">\n      <div style=\"margin-top: 18px\">퍼셉트론이 위의 그림과 같을 때 linear activation을 사용하여 전개해보면</div>\n      </div>\n   <img style=\"width: 300px; margin-right: 20px; margin-left: 18px; margin-top: 20px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?((xw_1+b_1)w_2+b_2)w_3+b_3\">\n   <img style=\"width: 340px; margin-right: 20px; margin-left: 18px; margin-top: 20px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?=(xw_1w_2+bw_2+b_2)w_3+b_3\">\n   <img style=\"width: 400px; margin-right: 20px; margin-left: 18px; margin-top: 20px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?=xw_1w_2w_3+b_1w_2w_3+b_2w_3+b_3\">\n          <div style=\"display: flex; margin-top: 30px; margin-left: 18px\">\n      <div style=\"margin-top: 18px\">가 나오는데</div>\n      </div>\n<div style=\"display: flex; margin-top: 30px; margin-left: 18px\">\n   <img style=\"width: 160px; margin-right: 20px; margin-left: 0px; margin-top: 8px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?w=w_1w_2w_3,\">\n   <img style=\"width: 260px; margin-right: 8px; margin-left: 0px; margin-top: 8px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?b=bw_2w_3+b_2w_3+b_3\">\n      <div style=\"margin-top: 18px\">로 치환하면</div>\n   </div>\n<div style=\"display: flex; margin-top: 10px; margin-left: 18px\">\n   <img style=\"width: 80px; margin-right: 8px; margin-left: 0px; margin-top: 8px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?aw+b\">\n   <div style=\"margin-top: 18px\">가 나오기 때문에 표현력이 선형 함수의 결합으로 제한된다.</div>\n   </div>\n<div id=\"sigmoid\"></div>\n<h2>sigmoid</h2>\n<img style=\"width: 100%;\" src=\"https://mblogthumb-phinf.pstatic.net/MjAyMDA3MDdfMTgx/MDAxNTk0MTMwODg2NzAw.Bgt42rm3pV0xTPfuVjN1UbXw9HchDcAdLdvnsrAQvJ0g.ILAv2yJkoMXNiWHKAUe0QswJWyr84GwwlRbXwxCogKUg.PNG.zzoyou_/sigmoid.png?type=w800\">\n<ol>\n<li>\n<p>전구간 미분 가능</p>\n</li>\n<li>\n<p>좀 더 부드러운 분류 가능</p>\n</li>\n<li>\n<p>확률로 해석 가능</p>\n</li>\n</ol>\n<h3>sigmoid 이용한 이진 분류</h3>\n<ul>\n<li>\n<p>고양이 강아지 분류의 예</p>\n<ul>\n<li>예측한 강아지 사진은 <img style=\"width: 50px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?q=1\">, 예측한 고양이 사진은 <img style=\"width: 50px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?q=0\">로 출력값이 나오고 정답은  <img style=\"width: 12px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?y\">로 정의</li>\n</ul>\n<p>즉, <img style=\"width: 140px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?q^y(1-q)^{1-y}\">를 키우면 된다.</p>\n<ul>\n<li>데이터를 넣는 것은 독립시행임으로 곱한 확률을 키우면 되는데 0~1사이의 값을 계속 곱하면 작아지게 된다.</li>\n</ul>\n<div style=\"display: flex;\">\n  <div style=\"margin-top: 3px;\">그러므로 log를 취해서</div> \n  <img style=\"width: 280px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?L=-\\sum_n log(q_n^{y_n}(1-q_n)^{y_n}))\">\n    <div style=\"margin-top: 3px;\">로 나타낸다.</div> \n</div>\n  <br>\n  *마이너스를 붙이는 이유 : 줄여야 하는 것이 Loss 함수 이므로\n<p>*로그를 취해도 상관없는 이유는 단조증가(monotonically increasing) - 줄이려는 방향이 같다.</p>\n<ul>\n<li>\n<p>logistic regression이라고 부름</p>\n<div style=\"display: flex;\">\n<div style=\"margin-top: 6px;\">logit을 linear regression 한 것 (logit이란 log-odds = </div> <img style=\"width: 100px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?log( \\frac{q}{1-q})\"><div style=\"margin-top: 6px;\">)</div>\n    </div>\n<br>\nlogit에서 q(확률)를 구하기 위해 역함수인 sigmoid를 통과시킨 것\n<p>(logit에서 q를 구하는 식으로 바꾸면 sigmoid 식과 같음)</p>\n</li>\n</ul>\n</li>\n</ul>\n<div id=\"log-likehood가 MSE 보다 나은 이유\"></div>\n<h2>log-likehood가 MSE 보다 나은 이유</h2>\n<p>MSE : <img style=\"width: 80px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?(q-1)^2\">을 minimize</p>\n<p>log-likehood : <img style=\"width: 66px; margin-right: 6px; margin-left: 8px; margin-top: 10px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?-log q\">을 minimize</p>\n<ol>\n<li>\n<p>그래프를 그려보면 log-likehood 더 민감</p>\n<img style=\"width: 50%; margin-right: 6px; margin-left: 8px; margin-top: 20px; margin-bottom: 20px;\" id=\"output\" src=\"/748471ff46cfafa29f57d62937abeffd/likehood.png\">\n</li>\n<li>\n<p>mse는 non-convex, log-likehood는 convex일 확률이 높다.</p>\n</li>\n</ol>\n<div id=\"딥러닝의 뿌리 이론\"></div>\n<h2>딥러닝의 뿌리 이론</h2>\n<blockquote>\n<p>MLE(Maximum Likelihood Estimation)</p>\n</blockquote>\n<p>*<img style=\"width: 140px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?p(y_i|f_w(x_i))\">을 최대로 하는 것</p>\n<ol>\n<li>mse : 가우시안 분포로 likelihood를 가정한 다음, <img style=\"width: 80px; margin-right: 6px; margin-left: 8px; margin-top: 16px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?f_w(x_i)\">의 출력을 평균값 <img style=\"width: 14px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?\\hat{y}\">로 삼고 NLL 식</li>\n<li>Cross-Entropy : 베르누이 분포로 로 likelihood를 가정한 다음, <img style=\"width: 80px; margin-right: 6px; margin-left: 8px; margin-top: 16px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?f_w(x_i)\">의 출력을 확률 <img style=\"width: 14px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?q\">로 삼고 NLL식 (다중 분류에서는 multinoulli(Categorical) 분포)</li>\n</ol>\n<p>*다중분류(softmax regression)  <img style=\"width: 120px; margin-right: 6px; margin-left: 8px; margin-top: 0px; margin-bottom: 0px;\" id=\"output\" src=\"https://latex.codecogs.com/svg.image?q_1^{y_1}q_2^{y_2}q_3^{y_3}\\cdot\\cdot\\cdot\">(y는 [1,0,0], [0,1,0], [0,0,1])</p>\n<p>*NLL : negative log-likelihood</p>","frontmatter":{"title":"인공지능 필수 기초 2탄","summary":"인공지능에 대한 필수 기초에 대해 총정리 하였습니다.","date":"2023.02.14.","categories":["AI BASIC"],"thumbnail":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAUCAIAAAAC64paAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEZklEQVR42j2TCU8bVxDH9wNUvRRE1IQkrTh87PpcY0OIACGaqmoKSJFCA4ratFVDOJoAbShpEVcSEQg4YByccBgaKBgM5rC963O959tdr20gBkSq9qv0EaRKT6N5evrp/5+ZN0j151/gRo0Vx8pseJnNbMP1Zr3GgBah6nyLCRsbs8di8VA4Go8nGJbnOEEQRAAkUZRhRCoqq2zFeiuOXraZS63GElxvMWiMmArV5BebsOf28TiViERiFEWfwjwPIC8ACSZISemV8itWo05lxQ3FZoNRrzWgapNOhUHYiI2POyAWjsTiFE1DmAenMDiFTSa8orxMpy2wmPTwGHVaVF2IaQpObBsxh2Pyf5h5B5/IChKMMEdQFCsrK1UXfWrUo2YDZtBpMG2RujBfq8rHT5THw+FIMBiCPJVgGIZjT8zzLMsmEjSiRXUohkKdYrPOYkRNek1RwaWi/M8uXcgzYWr7mH17eydEkhRFwXJFIAABqvMCz8WiEaSgUGOx4MUmFLbKgKrOnc358P33Pv7og/qaamF7WiCXiIA/CYS0LO0qyTeZdHYvc/hmL7ub5hNxJCc3r6KiHJrMO/cJqtXU1V7Lzc2xGlS+lwNLjgHfVN/KrDOTlAHLSDwn8pzEswoQUiIPr8iZM2fP513EcUtHZ/uGz3twuLeyvDjy8M7wr9+vTo8+aPlpYqj/aD8jMicwJNl4TEhQgGFiIRIxmS0XLhY8HHgKW5g9SO1m0qzknbf/3Hnr676OHwf/6PK4XUf7u1AqKfApwAMmARg6Hg75N31IZWXVjMsVIYJPno5u7mylFMGx0vR7X03r9er6St3zwVY6sg3JjAwykrCXFCHPURSXoOhoGKmtrYuQ/sNdJSPxrlevJhc7+53XG1v1N25it78z9A7WhcitKOGPEH7YIeVEmYLOZY6TORZpbm4Ok8FEggGi4vP5Oga+bH9c9e09w602rKnb+EvvtUAgGCLDm+veTe9KIhKiwmQkGIiHSBqOqqfnt/U1D8tJHJBpGnTea+gaqW5o0TXexWpuFH5z2wp/JMXKKx7v0p/uDc/Sltez5V3bXl+DCdLa0rSxuhwORYGcTqWzronH3aNXq2vUZlxVbCv4qh4jAuubo22+sVbvutc9O/vaPTczNbkw8/KvBTdSWVG+uvya8O/EKfbg8B8yTtzts/3QdrXxZr21RFNaef5+c0N7jXlu6AHD8AQZDRDheffC2PCQffgJXAzLYF+P17M8NzMN32iWcb5sn50f8a77Rp4NdXfcmbA/o1hp/+BvJbXLC7Isp3leJoio68UUYrRchiJOx/j87PTc9DTgBZFYySip/ezx4dHb47f/Zg+OFSUtAjkJByKl4Eq922qJoQHS2fXIpNfdb2ueck5srHpIMsrxopLa298/lJNpUUrCvYUxmcyIkgITjoO/i4MlxKI04l5YLbfqK0pMj/p7XzgmXE7n/MIiw0IgJQBoUpEkBQA4V8BxIsuJ8ImiGJKIbG36/wPG3qQZ7mlVkwAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/3672896cbb49cb2a4050c9744a3e8496/d89b0/test.jpg","srcSet":"/static/3672896cbb49cb2a4050c9744a3e8496/aaa13/test.jpg 374w,\n/static/3672896cbb49cb2a4050c9744a3e8496/b3c6b/test.jpg 748w,\n/static/3672896cbb49cb2a4050c9744a3e8496/d89b0/test.jpg 1496w","sizes":"(min-width: 1496px) 1496px, 100vw"},"sources":[{"srcSet":"/static/3672896cbb49cb2a4050c9744a3e8496/0dfd6/test.webp 374w,\n/static/3672896cbb49cb2a4050c9744a3e8496/8acfa/test.webp 748w,\n/static/3672896cbb49cb2a4050c9744a3e8496/06565/test.webp 1496w","type":"image/webp","sizes":"(min-width: 1496px) 1496px, 100vw"}]},"width":1496,"height":1496}},"publicURL":"/static/3672896cbb49cb2a4050c9744a3e8496/test.png"}}}}]}},"pageContext":{"slug":"/인공지능 필수 기초 2탄/"}},"staticQueryHashes":[],"slicesMap":{}}